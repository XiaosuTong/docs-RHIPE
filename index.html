<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <title>RHIPE Tutorial</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="">
    <meta name="author" content="">

    <link href="assets/bootstrap/css/bootstrap.css" rel="stylesheet">
    <link href="assets/custom/custom.css" rel="stylesheet">
    <!-- font-awesome -->
    <link href="assets/font-awesome/css/font-awesome.min.css" rel="stylesheet">

    <!-- prism -->
    <link href="assets/prism/prism.css" rel="stylesheet">
    <link href="assets/prism/prism.r.css" rel="stylesheet">
    <script type='text/javascript' src='assets/prism/prism.js'></script>
    <script type='text/javascript' src='assets/prism/prism.r.js'></script>
    
    
    
    <script type="text/javascript" src="assets/MathJax/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
   MathJax.Hub.Config({    
     extensions: ["tex2jax.js"],    
     "HTML-CSS": { scale: 100}    
   });
   </script>
    
    <!-- HTML5 shim, for IE6-8 support of HTML5 elements -->
    <!--[if lt IE 9]>
      <script src="js/html5shiv.js"></script>
    <![endif]-->
    
    <link href='http://fonts.googleapis.com/css?family=Lato' rel='stylesheet' type='text/css'>
    <!-- <link href='http://fonts.googleapis.com/css?family=Lustria' rel='stylesheet' type='text/css'> -->
    <link href='http://fonts.googleapis.com/css?family=Bitter' rel='stylesheet' type='text/css'>
    

    <!-- Fav and touch icons -->
    <link rel="apple-touch-icon-precomposed" sizes="144x144" href="ico/apple-touch-icon-144-precomposed.png">
    <link rel="apple-touch-icon-precomposed" sizes="114x114" href="ico/apple-touch-icon-114-precomposed.png">
      <link rel="apple-touch-icon-precomposed" sizes="72x72" href="ico/apple-touch-icon-72-precomposed.png">
                    <link rel="apple-touch-icon-precomposed" href="ico/apple-touch-icon-57-precomposed.png">
                                   <!-- <link rel="shortcut icon" href="ico/favicon.png"> -->
  </head>

  <body>

    <div class="container-narrow">

      <div class="masthead">
        <ul class="nav nav-pills pull-right">
           <li class='active'><a href='index.html'>Docs</a></li><li class=''><a href='functionref.html'>Function Ref</a></li><li><a href='https://github.com/tesseradata/RHIPE'>Github <i class='fa fa-github'></i></a></li>
        </ul>
        <p class="myHeader">RHIPE Tutorial</p>
      </div>

      <hr>

<div class="container-fluid">
   <div class="row-fluid">
   
   <div class="col-md-3 well">
   <ul class = "nav nav-list" id="toc">
   <li class='nav-header unselectable' data-edit-href='01.housing.Rmd'>Housing Data</li>
      
      <li class='active'>
         <a target='_self' class='nav-not-header' href='#intro'>Intro</a>
      </li>


      <li class='active'>
         <a target='_self' class='nav-not-header' href='#write-text-to-hdfs'>Write Text to HDFS</a>
      </li>


      <li class='active'>
         <a target='_self' class='nav-not-header' href='#read-and-divide-by-state'>Read and Divide by State</a>
      </li>


      <li class='active'>
         <a target='_self' class='nav-not-header' href='#compute-state--means'>Compute State  Means</a>
      </li>


      <li class='active'>
         <a target='_self' class='nav-not-header' href='#read-and-divide-by-county'>Read and Divide by County</a>
      </li>


      <li class='active'>
         <a target='_self' class='nav-not-header' href='#compute-county-means'>Compute County Means</a>
      </li>


      <li class='active'>
         <a target='_self' class='nav-not-header' href='#division-by-state-from-county-division'>Division by State from County Division</a>
      </li>


      <li class='active'>
         <a target='_self' class='nav-not-header' href='#access-subset-by-state'>Access Subset by State</a>
      </li>


      <li class='active'>
         <a target='_self' class='nav-not-header' href='#plots-by-state'>Plots by State</a>
      </li>


      <li class='active'>
         <a target='_self' class='nav-not-header' href='#manage-jobs'>Manage Jobs</a>
      </li>


<li class='nav-header unselectable' data-edit-href='05.performance.Rmd'>Tessera Cluster Logistic Performance Test</li>
      
      <li class='active'>
         <a target='_self' class='nav-not-header' href='#introduction'>Introduction</a>
      </li>


      <li class='active'>
         <a target='_self' class='nav-not-header' href='#generate-datasets'>Generate Datasets</a>
      </li>


      <li class='active'>
         <a target='_self' class='nav-not-header' href='#elapsed-time-measurement'>Elapsed Time Measurement</a>
      </li>


      <li class='active'>
         <a target='_self' class='nav-not-header' href='#r-code----set-up'>R code -- Set Up</a>
      </li>


      <li class='active'>
         <a target='_self' class='nav-not-header' href='#r-code----generate-dataset'>R code -- Generate Dataset</a>
      </li>


      <li class='active'>
         <a target='_self' class='nav-not-header' href='#r-code----timing'>R code -- Timing</a>
      </li>


      <li class='active'>
         <a target='_self' class='nav-not-header' href='#r-code----save-the-results'>R code -- Save the Results</a>
      </li>


      <li class='active'>
         <a target='_self' class='nav-not-header' href='#results'>Results</a>
      </li>


      <li class='active'>
         <a target='_self' class='nav-not-header' href='#visualize-the-results'>Visualize the Results</a>
      </li>

   </ul>
   </div>

<div class="col-md-9 tab-content" id="main-content">

<div class='tab-pane active' id='intro'>
<h3>Intro</h3>

<p>We&#39;ll demonstrate the <code>RHIPE</code> package with an analysis of housing data.
The housing data set contains monthly median list and selling prices per square foot and number 
of units sold for 2,883 counties in 49 states from October 2008 to March 2014, harvested from 
Quandl&#39;s Zillow Housing Data.</p>

<p>Here are the first few lines of the raw text file:</p>

<pre><code>06001,Alameda,CA,2008-10-01,NA,307.97872340425,325.8118
06001,Alameda,CA,2008-11-01,NA,299.16666666667,NA
06001,Alameda,CA,2008-11-01,NA,NA,318.115
06001,Alameda,CA,2008-12-01,NA,289.88149498633,305.7878
06001,Alameda,CA,2009-01-01,NA,288.5,291.5977
</code></pre>

<p>There are 224,369 rows and 7 fields. Each field is separated by a comma, and there is no header row.
The meaning of each field is as follows:</p>

<ul>
<li><strong>fips</strong>: FIPS county code</li>
<li><strong>county</strong>: county name</li>
<li><strong>state</strong>: state abbreviation</li>
<li><strong>date</strong>: date in &quot;YYYY-MM-DD&quot; format.  Each month&#39;s values are placed at the first day of the month</li>
<li><strong>units</strong>: number of units sold</li>
<li><strong>list</strong>: monthly median list price dollar per square foot</li>
<li><strong>selling</strong>: monthly median selling price dollar per square foot</li>
</ul>

<p>The total size of the text file is around 12MB. It is available as 
<code>housing.txt</code> in our Tesseradata Github repository of the <code>RHIPE</code> documentation 
<a href="https://raw.githubusercontent.com/xiaosutong/docs-RHIPE/gh-pages/housing.txt">here</a>.   </p>

<p>Note:</p>

<p>A housing unit is a house, an apartment, a mobile home, a group of rooms, or a single room that is
occupied (or if vacant, is intended for occupancy) as separate living quarters. Separate living 
quarters are those in which the occupants live and eat separately from any other persons in the 
building and which have direct access from the outside of the building or through a common hall.</p>

</div>


<div class='tab-pane' id='write-text-to-hdfs'>
<h3>Write Text to HDFS</h3>

<p>Begin by connecting via ssh to the machine from which you will issue commands to your HDFS.  This might
be the frontend machine of your cluster, or it may be one of the Hadoop nodes.  Navigate to a directory
where you can store files, such as your .RData and .Rhistory files.  This might be your home directory.
Start an interactive R session.</p>

<p>You can double-check the name of your local working directory with the <code>getwd</code> command.  Mine is
called &quot;/home/median/u41/tongx&quot; but yours will be different.</p>

<pre><code class="r">getwd()
</code></pre>

<pre><code>[1] &quot;/home/median/u41/tongx&quot;
</code></pre>

<p>Download the data file to your local working directory with the following command:</p>

<pre><code class="r">system(&quot;wget https://raw.githubusercontent.com/xiaosutong/docs-RHIPE/gh-pages/housing.txt&quot;)
</code></pre>

<p>If it downloaded properly, then &quot;housing.txt&quot; will show up in the output of this command, which lists files
in your local working directory:</p>

<pre><code class="r">list.files(&quot;.&quot;)
</code></pre>

<p>This tutorial assumes that you&#39;ve already installed Rhipe using the instructions provided.
Every time we use Rhipe, we have to call the Rhipe library in R and initialize it.  Your values
for <code>zips</code> and <code>runner</code> might be different than these, depending on the details of your installation.</p>

<pre><code class="r">library(Rhipe)
rhinit()
rhoptions(zips = &quot;/ln/share/RhipeLib.tar.gz&quot;)
rhoptions(runner = &quot;sh ./RhipeLib/library/Rhipe/bin/RhipeMapReduce.sh&quot;)
</code></pre>

<p>Now we want to copy the raw text file to the HDFS.  The function that writes files to HDFS is <code>rhput()</code>.<br>
Replace <code>tongx</code> with an appropriate HDFS directory, such as your user name.</p>

<pre><code class="r">rhput(&quot;./housing.txt&quot;, &quot;/ln/tongx/housing/housing.txt&quot;)
</code></pre>

<p>The <code>rhput</code> function takes two arguments.
The first argument is the path to the local file to be copied, and the second argument is the HDFS path where
the file will be written. <code>rhput</code> creates the file at destination, overwriting the destination if
it already exists.  We can also copy files onto HDFS via Hadoop&#39;s command line interface, but
Rhipe allows us to achieve this task from within R.</p>

<p>We can confirm that the housing data text file has been written to HDFS with the <code>rhexists</code> function.
Make sure you specify the same directory as you used in the last step.</p>

<pre><code class="r">rhexists(&quot;/ln/tongx/housing/housing.txt&quot;)
</code></pre>

<pre><code>[1] TRUE
</code></pre>

<p>If we want to see more details about a file or directory on HDFS, we can use <code>rhls()</code>.</p>

<pre><code class="r">rhls(&quot;/ln/tongx/housing&quot;)
</code></pre>

<pre><code>  permission owner      group     size          modtime                            file
1 -rw-rw-rw- tongx supergroup 11.82 mb 2014-09-17 11:11   /ln/tongx/housing/housing.txt
</code></pre>

<p><code>rhls()</code> is very similar to the bash command <code>ls</code>.  It will list all content under a given address. 
We can see that the <code>housing.txt</code> file with size 11.82Mb is located under <code>/user/tongx/housing/</code> 
on HDFS.</p>

<p>With our data on the HDFS, we are ready to start a D&amp;R analysis.</p>

</div>


<div class='tab-pane' id='read-and-divide-by-state'>
<h3>Read and Divide by State</h3>

<p>The first step in a D&amp;R analysis is to choose a division method and create subsets.  We&#39;ll
divide the housing data by state.  Rhipe allows us to read the raw text file on HDFS and 
create a copy of the data broken into subsets using a MapReduce job.  The original data file will remain on HDFS.
The data within each subset will be stored as R objects, rather than raw text.  This will 
allow us to apply subsequent data analytic and visual methods using familiar R commands.</p>

<p>In the MapReduce framework, we keep track of subsets as key-value pairs.  The key is a identifier for
the subset.  It&#39;s usually a small data object, like a single number or character string.  There
can be multiple subsets that share a common key.  The value is where we store the data.  For 
example, it may be an R data frame.</p>

<p>The map is a function applied to every key-value pair.  Its output is one or more key-value
pairs, where the key may or may not have changed from the input key.  The output key-value 
pairs of the map are sometimes called intermediate key-value pairs, because they will be the
input to the reduce function.  The intermediate key-value pairs are grouped by key, and then
the reduce function is applied to each group.  The output of the reduce function is again
one or more key-value pairs.  Rhipe allows us to write the map and reduce functions in
R code, using R objects for the keys and values.</p>

<p>A valid MapReduce job in Rhipe consists of a map expression, an optional reduce expression, 
and an execution function <code>rhwatch()</code>.  If no reduce expression is given, the output of the
map is the final output.</p>

<h4>Map</h4>

<pre><code class="r">map1 &lt;- expression({
  lapply(seq_along(map.keys), function(r) {
    line = strsplit(map.values[[r]], &quot;,&quot;)[[1]]
    key &lt;- line[3]
    value &lt;- as.data.frame(rbind(line[-3]), stringsAsFactors = FALSE)
    names(value) &lt;- c(
      &quot;fips&quot;, &quot;county&quot;, &quot;date&quot;, 
      &quot;units&quot;, &quot;list&quot;, &quot;selling&quot;
    )
    value$list &lt;- as.numeric(value$list)
    value$selling &lt;- as.numeric(value$selling)
    value$units &lt;- as.numeric(value$units)
    rhcollect(key, value)
  })
})
</code></pre>

<p>The input keys are line numbers in the housing data text file.  They are the elements of the list
object <code>map.keys</code>.  The input values are the lines of text, which are the elements of the list object <code>map.values</code>.
This is the default when the input is a raw text file.  For each input key-value pair, <code>rhcollect</code> emits an
intermediate key-value pair, where the key is the state name (the third field in the comma-separated
line) and the value is all other fields in the line, stored as a data frame with a single row.</p>

<h4>Reduce</h4>

<pre><code class="r">reduce1 &lt;- expression(
  pre = {
    oneState &lt;- data.frame()
  },
  reduce = {
    oneState &lt;- rbind(oneState, do.call(rbind, reduce.values))
  },
  post = {
    attr(oneState, &quot;state&quot;) &lt;- reduce.key
    rhcollect(reduce.key, oneState)
  }
)
</code></pre>

<p>Between the map and reduce stages, the intermediate key-value pairs are grouped by key (the state name).
The current group&#39;s key is available in the object <code>reduce.key</code>, and all values associated with that key
are elements of the list object <code>reduce.values</code>.  The reduce expression has three parts: <code>pre</code>, which is 
executed once first; <code>reduce</code>, which is executed repeatedly until all intermediate values associated with
the current key have been processed; and <code>post</code>, which is executed once at the end.  </p>

<p>In the reduce expression above, our goal is to combine all observations associated with one particular
state into a single data frame.  In <code>pre</code>, we initialize an empty data frame, <code>oneState</code>.  In <code>reduce</code>, 
we use <code>rbind</code>
to combine all observations associated with one particular state.  In <code>post</code>, we add an attribute to the
data frame containing the state name and emit the final key-value
pair.  The key is the state name, and the value is the data frame with all observations belonging to
that state.  These final key-value pairs are written to HDFS, and will persist for subsequent analyses.</p>

<h4>Execution Function</h4>

<pre><code class="r">mr1 &lt;- rhwatch(
  map      = map1,
  reduce   = reduce1,
  input    = rhfmt(&quot;/ln/tongx/housing/housing.txt&quot;, type = &quot;text&quot;),
  output   = rhfmt(&quot;/ln/tongx/housing/byState&quot;, type = &quot;sequence&quot;),
  mapred   = list(
    mapred.reduce.tasks = 10
  ),
  readback = FALSE
)
</code></pre>

<p>The <code>rhwatch</code> function packages and executes our Rhipe MapReduce job.  In addition to the <code>map</code> and
<code>reduce</code> expressions created above, we specify the HDFS locations of the input and output for this
MapReduce job.  The input is the location where we stored our raw text file using <code>rhput</code> in the
previous section.  The output is any location on HDFS we choose.  Be careful, as any existing data
in the output location will be overwritten.</p>

<p>The <code>mapred</code> argument contains optional configuration parameters for the MapReduce job.  In this
case, we&#39;ve specified 10 reduce tasks using <code>mapred.reduce.tasks</code>.  This means that of the 49 groups
of key-value pairs corresponding to the 49 states in our data set, 10 at a time will be processed
in parallel.  Specifying 10 reduce tasks also means that the output written to HDFS will be broken
into 10 files (we&#39;ll come back to this point in the next section).  Finally, <code>readback = FALSE</code> 
tells Rhipe not to read the final output from HDFS into our interactive R session.  We&#39;ll do that
with a separate command. </p>

<pre><code>Saving 1 parameter to /tmp/rhipe-temp-params-bbb96e029776c9953476a54c74d9eaf7 (use rhclean to delete all temp files)
[Thu Sep 18 22:25:52 2014] Name:2014-09-18 22:25:52 Job: job_201405301308_4709  State: PREP Duration: 0.203
URL: http://deneb.stat.purdue.edu:50030/jobdetails.jsp?jobid=job_201405301308_4709
       pct numtasks pending running complete killed failed_attempts killed_attempts
map      0        1       1       0        0      0               0               0
reduce   0       10      10       0        0      0               0               0
Waiting 5 seconds
[Thu Sep 18 22:25:57 2014] Name:2014-09-18 22:25:52 Job: job_201405301308_4709  State: RUNNING Duration: 5.24
URL: http://deneb.stat.purdue.edu:50030/jobdetails.jsp?jobid=job_201405301308_4709
       pct numtasks pending running complete killed failed_attempts killed_attempts
map      0        1       0       1        0      0               0               0
reduce   0       10      10       0        0      0               0               0
Waiting 5 seconds
</code></pre>

<p>After our job has completed successfully, the output will be in the location we specified on the HDFS.
Since this data set is quite small, we can read the whole thing from HDFS into our interactive R
environment using <code>rhread</code>.  All we have to specify is the HDFS location we wish to read from.</p>

<pre><code class="r">stateSubsets &lt;- rhread(&quot;/ln/tongx/housing/byState&quot;)
</code></pre>

<pre><code>Read 49 objects(13.52 MB) in 1.41 seconds
</code></pre>

<p>Rhipe conveniently packages the key-value pairs in our HDFS output location as a nested list, which we&#39;ve
assigned to the variable <code>stateSubsets</code>.  Since there were 49 key-value pairs in the output of our reduce
stage, there are 49 elements in <code>stateSubsets</code>.  Each element is itself a list with two elements: a key
and a value.  In this case, the keys are character strings containing the state names, and the 
values are data frames, just as they should be based on our reduce code.</p>

<p>We can take a look at the data frame contained in the first key-value pair.  The key-value pairs
are in no particular order.    </p>

<pre><code class="r">head(stateSubsets[[1]][[2]])
</code></pre>

<p>We can also look at the structure of <code>stateSubsets</code> and confirm that it&#39;s what we expected.</p>

<pre><code class="r">str(stateSubsets)
</code></pre>

<pre><code>List of 49
 $ :List of 2
  ..$ : chr &quot;WV&quot;
  ..$ :&#39;data.frame&#39;:    3836 obs. of  6 variables:
  .. ..$ fips   : chr [1:3836] &quot;54015&quot; &quot;54015&quot; &quot;54015&quot; &quot;54015&quot; ...
  .. ..$ county : chr [1:3836] &quot;Clay&quot; &quot;Clay&quot; &quot;Clay&quot; &quot;Clay&quot; ...
  .. ..$ time   : chr [1:3836] &quot;2012-12-01&quot; &quot;2012-11-01&quot; &quot;2012-10-01&quot; &quot;2012-09-01&quot; ...
  .. ..$ sold   : chr [1:3836] &quot;NA&quot; &quot;NA&quot; &quot;NA&quot; &quot;NA&quot; ...
  .. ..$ list   : num [1:3836] 78.5 80 80 80.1 93.1 ...
  .. ..$ selling: num [1:3836] NA NA NA NA NA NA NA NA NA NA ...
 $ :List of 2
  ..$ : chr &quot;KY&quot;
  ..$ :&#39;data.frame&#39;:    8059 obs. of  6 variables:
  .. ..$ fips   : chr [1:8059] &quot;21235&quot; &quot;21235&quot; &quot;21235&quot; &quot;21235&quot; ...
  .. ..$ county : chr [1:8059] &quot;Whitley&quot; &quot;Whitley&quot; &quot;Whitley&quot; &quot;Whitley&quot; ...
  .. ..$ time   : chr [1:8059] &quot;2012-05-01&quot; &quot;2012-07-01&quot; &quot;2012-08-01&quot; &quot;2012-09-01&quot; ...
  .. ..$ sold   : chr [1:8059] &quot;NA&quot; &quot;NA&quot; &quot;NA&quot; &quot;NA&quot; ...
  .. ..$ list   : num [1:8059] 73.2 74.8 74.1 74.8 77.2 ...
  .. ..$ selling: num [1:8059] NA NA NA NA NA NA NA NA NA NA ...
......
 $ :List of 2
  ..$ : chr &quot;TX&quot;
  ..$ :&#39;data.frame&#39;:    13108 obs. of  6 variables:
  .. ..$ fips   : chr [1:13108] &quot;48215&quot; &quot;48479&quot; &quot;48479&quot; &quot;48479&quot; ...
  .. ..$ county : chr [1:13108] &quot;Hidalgo&quot; &quot;Webb&quot; &quot;Webb&quot; &quot;Webb&quot; ...
  .. ..$ time   : chr [1:13108] &quot;2009-05-01&quot; &quot;2011-11-01&quot; &quot;2011-12-01&quot; &quot;2012-01-01&quot; ...
  .. ..$ sold   : chr [1:13108] &quot;NA&quot; &quot;NA&quot; &quot;NA&quot; &quot;NA&quot; ...
  .. ..$ list   : num [1:13108] 76.3 83.6 84.7 85.2 86.4 ...
  .. ..$ selling: num [1:13108] NA NA NA NA NA NA NA NA NA NA ...
 $ :List of 2
  ..$ : chr &quot;WA&quot;
  ..$ :&#39;data.frame&#39;:    3705 obs. of  6 variables:
  .. ..$ fips   : chr [1:3705] &quot;53049&quot; &quot;53049&quot; &quot;53049&quot; &quot;53049&quot; ...
  .. ..$ county : chr [1:3705] &quot;Pacific&quot; &quot;Pacific&quot; &quot;Pacific&quot; &quot;Pacific&quot; ...
  .. ..$ time   : chr [1:3705] &quot;2012-08-01&quot; &quot;2012-09-01&quot; &quot;2012-09-01&quot; &quot;2012-10-01&quot; ...
  .. ..$ sold   : chr [1:3705] &quot;NA&quot; &quot;29&quot; &quot;NA&quot; &quot;NA&quot; ...
  .. ..$ list   : num [1:3705] 120 NA 125 NA 120 ...
  .. ..$ selling: num [1:3705] NA 94.6 NA 90.8 NA ...
</code></pre>

<p>We&#39;ve now successfully divided our data into subsets and stored each subset as an R object.<br>
They will persist on HDFS and be used for many analytic methods, each applied using Rhipe. </p>

</div>


<div class='tab-pane' id='compute-state--means'>
<h3>Compute State  Means</h3>

<p>Now we&#39;ll apply an analytic method to the subsets of our housing data.  Our analytic
method will be to take the mean.  We have monthly list prices and sale prices per square foot, 
so our desired output is two numbers for each state: a mean of list prices and a mean 
of sale prices.  The recombination step will be to put all the state means in a single data
frame, which can be further analyzed in an interactive R session.</p>

<p>We apply our chosen analytic method with a second Rhipe MapReduce job:</p>

<h4>Map</h4>

<pre><code class="r">map2 &lt;- expression({
  lapply(seq_along(map.keys), function(r) {
    value &lt;- data.frame(
      state = attr(map.values[[r]], &quot;state&quot;),
      listMean = mean(map.values[[r]]$list, na.rm = TRUE),
      saleMean = mean(map.values[[r]]$selling, na.rm = TRUE),
      stringsAsFactors = FALSE
    )
    rhcollect(1, value)
  })
})
</code></pre>

<p>The input to our map is the the final output we created in the previous section.  The keys 
were state names, stored as character objects, and the values were data frames with prices,
dates, and county information.  In general, 
keys need not be unique, but in this case we know that there are 49 state names, where no state
is duplicated.  </p>

<p>To each state&#39;s data frame, we apply the same action: take the mean of the list price and 
the mean of the sale price.  These are stored in a single row data frame to be recombined in 
the reduce step below.  The row has three columns: state name plus the two calculated means.
As before, <code>rhcollect</code> emits intermediate key-value pairs.  One key-value pair is emitted per
state.  The value is the single row data frame we just created.  We use the same key for all 49
states so that they will all appear together in the reduce step.  The key itself is meaningless,
so we&#39;ll use 1 as a placeholder.</p>

<h4>Reduce</h4>

<pre><code class="r">reduce2 &lt;- expression(
  pre = {
    allMeans &lt;- data.frame()
  },
  reduce = {
    allMeans &lt;- rbind(allMeans, do.call(rbind, reduce.values))
  },
  post = {
    rhcollect(reduce.key, allMeans)
  }
)
</code></pre>

<p>We divided our data into subsets and applied an analytic method to each subset.  The reduce step
above is our recombination.  We recombine the state means into a data frame with 49 rows and 3
columns.  Later, we can read that data frame from HDFS into our interactive R session for further
analysis.</p>

<p>The input key is the placeholder value 1, which is left unchanged as the output key.  The input
values are the single row data frames for each state, and the output values is the 49 row data 
frame with all states represented.  In the <code>pre</code> step we initialize an empty data frame.  In
<code>reduce</code> we use <code>rbind</code> to append the single row data frames to the empty data frame we created, 
and in <code>post</code> we use <code>rhcollect</code> to emit the final output so that it will be written to HDFS.</p>

<h4>Execution Function</h4>

<p>In <code>rhwatch()</code> this time, we&#39;ve changed several arguments. First, in the <code>rhfmt</code> of <code>input</code> argument,
<code>type</code> is specified to be &quot;sequence&quot;, since the input file to this mapreduce job is the output
from our division. This indicates to Rhipe that the input is not a raw text file, but rather a 
file already organized as key-value pairs.  Also we request 5 reduce tasks for this job using
the <code>mapred.reduce.tasks</code> option.  Finally, we assign
<code>readback</code> to be <code>TRUE</code>. By doing this, the final results not only will be saved on HDFS, but also
will be read back from HDFS (without using <code>rhread()</code>) and assigned to an object in our interactive R
session.  We&#39;ve named that object <code>stateMeans</code>.</p>

<pre><code class="r">stateMeans &lt;- rhwatch(
  map      = map2,
  reduce   = reduce2,
  input    = rhfmt(&quot;/ln/tongx/housing/byState&quot;, type = &quot;sequence&quot;),
  output   = rhfmt(&quot;/ln/tongx/housing/meanByState&quot;, type = &quot;sequence&quot;),
  mapred   = list( 
    mapred.reduce.tasks = 5
  ),
  readback = TRUE
)
</code></pre>

<p>Just as before, once we submit the job, we see job status information.</p>

<pre><code>[Thu Sep 18 23:48:19 2014] Name:2014-09-18 23:48:19 Job: job_201405301308_4715  State: PREP Duration: 0.175
URL: http://deneb.stat.purdue.edu:50030/jobdetails.jsp?jobid=job_201405301308_4715
       pct numtasks pending running complete killed failed_attempts killed_attempts
map      0       10      10       0        0      0               0               0
reduce   0        5       5       0        0      0               0               0
Waiting 5 seconds
[Thu Sep 18 23:48:24 2014] Name:2014-09-18 23:48:19 Job: job_201405301308_4715  State: RUNNING Duration: 5.206
URL: http://deneb.stat.purdue.edu:50030/jobdetails.jsp?jobid=job_201405301308_4715
       pct numtasks pending running complete killed failed_attempts killed_attempts
map      0       10       0      10        0      0               0               0
reduce   0        5       5       0        0      0               0               0
Waiting 5 seconds
.......
Read 1 objects(2.42 KB) in 0.06 seconds
</code></pre>

<pre><code class="r">str(stateMeans)
</code></pre>

<pre><code>List of 1
 $ :List of 2
  ..$ : num 1
  ..$ :&#39;data.frame&#39;:  49 obs. of  3 variables:
  .. ..$ state      : Factor w/ 49 levels &quot;IA&quot;,&quot;KS&quot;,&quot;AZ&quot;,..: 1 2 3 4 5 6 7 8 9 10 ...
  .. ..$ listmean   : num [1:49] 74.5 66 110.6 369.1 151 ...
  .. ..$ sellingmean: num [1:49] 95.1 NaN 101.3 422.5 143.4 ...
</code></pre>

<p>As we can see, the result is a list of length 1.  This make sense, since we only output a single
key-value pair.  This single element is itself a list of length 2: one element for the key, and 
another for the value.  The value is what we&#39;re interested in, namely, the data frame of 49 state means.
It has three columns which are state abbreviation, mean of median list price per square feet, and mean of 
median sale price per square feet.</p>

<pre><code class="r">stateMeans &lt;- stateMeans[[1]][[2]]
head(stateMeans)
</code></pre>

<pre><code>   state listMean    saleMean
4     DC 369.0546    422.4744
45    MA 259.1316    210.8714
44    CA 192.6343    185.8716
10    RI 188.9953    174.9670
46    NJ 180.0136    171.4495
22    CT 162.4686    151.7553
</code></pre>

<p>Let&#39;s take a look at how the output was stored on HDFS using <code>rhls</code>.</p>

<pre><code class="r">rhls(&quot;/ln/tongx/housing/meanByState&quot;)
</code></pre>

<pre><code>  permission owner      group        size          modtime                                         file
1 -rw-r--r-- tongx supergroup           0 2014-09-18 23:56     /ln/tongx/housing/meanByState/_SUCCESS
2 drwxrwxrwx tongx supergroup           0 2014-09-18 23:56        /ln/tongx/housing/meanByState/_logs
3 -rw-r--r-- tongx supergroup    1.363 kb 2014-09-18 23:56 /ln/tongx/housing/meanByState/part-r-00000
4 -rw-r--r-- tongx supergroup    94 bytes 2014-09-18 23:56 /ln/tongx/housing/meanByState/part-r-00001
5 -rw-r--r-- tongx supergroup    94 bytes 2014-09-18 23:56 /ln/tongx/housing/meanByState/part-r-00002
6 -rw-r--r-- tongx supergroup    94 bytes 2014-09-18 23:56 /ln/tongx/housing/meanByState/part-r-00003
7 -rw-r--r-- tongx supergroup    94 bytes 2014-09-18 23:56 /ln/tongx/housing/meanByState/part-r-00004
</code></pre>

<p>There are five files in &quot;/ln/tongx/housing/meansByState&quot;, named from &quot;part-r-00000&quot; to 
&quot;part-r-00004&quot;. There are five of these because we selected 5 reduce tasks with <code>mapred.reduce.tasks</code>.
Besides these five files, there are another two files named &quot;_SUCCESS&quot; and 
&quot;_logs&quot; which record the metadata and log information. </p>

<p>Notice that four of the five files have the same size, 94 bytes, which is quite small. 
This is because those four files are 
empty. Since we only had one intermediate key-value pair (the output of the map), four of the 
reduce tasks we requested did nothing.</p>

<p>We&#39;ve now successfully completed our first D&amp;R analysis with Rhipe.  We created a division by state, applied an
analytic method to each subset when we took the mean, and recombined the subset outputs into an R
data frame for further interactive analysis.  We did the division in one Rhipe MapReduce job, and 
the analytic method and recombination in a second job.</p>

</div>


<div class='tab-pane' id='read-and-divide-by-county'>
<h3>Read and Divide by County</h3>

<p>As we showed previously, we divided the whole data set to subsets by state, and then calculated the mean list
and selling price for each state. According to different purpose of the data analysis, we sometime would
like to divide the data set by diferent variables. So we are going to show that what if we divide our housing
data by county from the text file as well.</p>

<p>The input keys are line numbers in the housing data text file.  They are the elements of the list
object <code>map.keys</code>.  The input values are the lines of text, which are the elements of the list object <code>map.values</code>.
This is the default when the input is a raw text file.  For each input key-value pair, <code>rhcollect</code> emits an
intermediate key-value pair, where the key is the state name (the third field in the comma-separated
line) and the value is all other fields in the line, stored as a data frame with a single row.</p>

<h4>Map</h4>

<pre><code class="r">map3 &lt;- expression({
  lapply(seq_along(map.keys), function(r) {
    line = strsplit(map.values[[r]], &quot;,&quot;)[[1]]
    key &lt;- line[1:3]
    value &lt;- as.data.frame(rbind(line[c(-1, -2, -3)]), stringsAsFactors = FALSE)
    names(value) &lt;- c(&quot;date&quot;, &quot;units&quot;, &quot;list&quot;, &quot;selling&quot;)
    value$list &lt;- as.numeric(value$list)
    value$selling &lt;- as.numeric(value$selling)
    rhcollect(key, value)
  })
})
</code></pre>

<p>Note that we want to use the FIPS code as a unique identifier for the county, since counties in
different states can share a common name.  This time we&#39;ve used a character vector of length 3
as the key.  It contains the unique FIPS code, the county name, and the state name.</p>

<h4>Reduce</h4>

<pre><code class="r">reduce3 &lt;- expression(
  pre = {
    oneCounty &lt;- data.frame()
  },
  reduce = {
    oneCounty &lt;- rbind(oneCounty, do.call(rbind, reduce.values))
  },
  post = {
    attr(oneCounty, &quot;FIPS&quot;) &lt;- reduce.key[1]
    attr(oneCounty, &quot;county&quot;) &lt;- reduce.key[2]
    attr(oneCounty, &quot;state&quot;) &lt;- reduce.key[3]
    rhcollect(reduce.key, oneCounty)
  }
)
</code></pre>

<p>By removing the FIPS, county, and state columns from the data frame and storing them as
attributes, we&#39;ve eliminated redundant information in each data frame. Working with massive 
data sets, we want our data to take up the least possible space on disk in order to save 
read/write time.</p>

<h4>Execution Function</h4>

<pre><code class="r">mr3 &lt;- rhwatch(
  map      = map3,
  reduce   = reduce3,
  input    = rhfmt(&quot;/ln/tongx/housing/housing.txt&quot;, type = &quot;text&quot;),
  output   = rhfmt(&quot;/ln/tongx/housing/byCounty&quot;, type = &quot;sequence&quot;),
  mapred   = list(
    mapred.reduce.tasks = 10
  ),
  readback = FALSE
)
</code></pre>

<p>After the job completes successfully, we&#39;ll read the results from HDFS into our interactive R 
session as we did before.  This time, let&#39;s use the <code>max</code> argument to <code>rhread</code>, which specifies 
how many key-value pairs to read. The default value is -1, which means read in all key-value pairs.</p>

<pre><code class="r">countySubsets &lt;- rhread(&quot;/ln/tongx/housing/byCounty&quot;, max = 10)
</code></pre>

<pre><code>Read 10 objects(31.39 KB) in 0.04 seconds
</code></pre>

<p>Suppose we want to see all 10 keys that we read.  Recall that key-value pairs are stored as a nested
list.  So what we want is the first element of each element in the list.  We can use <code>lapply</code> to get
them:</p>

<pre><code class="r">keys &lt;- unlist(lapply(countySubsets, &quot;[[&quot;, 1))
keys
</code></pre>

<pre><code> [1] &quot;01013&quot; &quot;01031&quot; &quot;01059&quot; &quot;01077&quot; &quot;01095&quot; &quot;01103&quot; &quot;01121&quot; &quot;04001&quot; &quot;05019&quot; &quot;05037&quot;
</code></pre>

<p>Finally, let&#39;s check that we have the FIPS code, state name, and county name information saved as 
attributes of the data frame.</p>

<pre><code class="r">attributes(countySubsets[[1]][[2]])
</code></pre>

<pre><code>$names
[1] &quot;time&quot;             &quot;sold&quot;            &quot;list&quot;             &quot;selling&quot;

$row.names
 [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32
[33] 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 
[65] 65 66

$state
[1] &quot;AL&quot;

$county
[1] &quot;Butler&quot;

$class
[1] &quot;data.frame&quot;
</code></pre>

</div>


<div class='tab-pane' id='compute-county-means'>
<h3>Compute County Means</h3>

<p>We calculate the mean list price and mean sale price by county exactly the same way we 
calculated them by state. </p>

<h4>Map</h4>

<p>In the map expression, we create a single row data frame for each county. The data frame has five
columns: FIPS, listMean, saleMean, state, and county name. FIPS code, state name, and county name 
can be found in the attributes of each element of <code>map.values</code>. In order to combine all means by
county into one data frame, we will assign 1 to be the key for all intermediate key-value pairs.</p>

<pre><code class="r">map4 &lt;- expression({
  lapply(seq_along(map.keys), function(r) {
    value &lt;- data.frame(
      listMean = mean(map.values[[r]]$list, na.rm = TRUE),
      saleMean = mean(map.values[[r]]$selling, na.rm = TRUE),
      stringsAsFactors = FALSE
    )
    value$state &lt;- attr(map.values[[r]], &quot;state&quot;)
    value$county &lt;- attr(map.values[[r]], &quot;county&quot;)
    value$FIPS &lt;- attr(map.values[[r]], &quot;FIPS&quot;)
    rhcollect(1, value)
  })
})
</code></pre>

<h4>Reduce</h4>

<p>We can use the same reduce expression which we used to find the means by state. The final output
consists of one key-value pair, where the key is 1, and value is the data frame with all county
means.</p>

<pre><code class="r">reduce4 &lt;- expression(
  pre = {
    countyMeans &lt;- data.frame()
  },
  reduce = {
    countyMeans &lt;- rbind(countyMeans, do.call(rbind, reduce.values))
  },
  post = {
    rhcollect(reduce.key, countyMeans)
  }
)
</code></pre>

<h4>Execution Function</h4>

<pre><code class="r">meansByCounty &lt;- rhwatch(
  map      = map4,
  reduce   = reduce4,
  input    = rhfmt(&quot;/ln/tongx/housing/byCounty&quot;, type = &quot;sequence&quot;),
  output   = rhfmt(&quot;/ln/tongx/housing/meansByCounty&quot;, type = &quot;sequence&quot;),
  mapred   = list( 
    mapred.reduce.tasks = 1
  ),
  readback = TRUE
)
</code></pre>

<p>This time we specify only one reduce task by setting <code>mapred.reduce.tasks</code> to be 1.
Since we know there is only one
key-value pair in the output, we only need one output file. Eliminating unnecessary output files 
can speed up this job and future jobs which read its output.</p>

<pre><code class="r">str(meansByCounty)
</code></pre>

<pre><code>List of 1
 $ :List of 2
  ..$ : num 1
  ..$ :&#39;data.frame&#39; :    2883 obs. of  5 variables:
  .. ..$ fips       : chr [1:2883] &quot;01005&quot; &quot;01023&quot; &quot;01041&quot; &quot;01069&quot; ...
  .. ..$ listmean   : num [1:2883] 85.5 54.3 59.3 87.6 74.4 ...
  .. ..$ sellingmean: num [1:2883] NaN NaN NaN NaN NaN ...
  .. ..$ state      : chr [1:2883] &quot;AL&quot; &quot;AL&quot; &quot;AL&quot; &quot;AL&quot; ...
  .. ..$ county     : chr [1:2883] &quot;Barbour&quot; &quot;Choctaw&quot; &quot;Crenshaw&quot; &quot;Houston&quot; ...
</code></pre>

<pre><code class="r">head(meansByCounty[[1]][[2]])
</code></pre>

<pre><code>   FIPS listMean  saleMean state       county
1 01005 85.46983       NaN    AL      Barbour
2 01023 54.27378       NaN    AL      Choctaw
3 01041 59.29840       NaN    AL     Crenshaw
4 01069 87.64237       NaN    AL      Houston
5 01087 74.35722       NaN    AL        Macon
6 01113 70.92100       NaN    AL      Russell
</code></pre>

</div>


<div class='tab-pane' id='division-by-state-from-county-division'>
<h3>Division by State from County Division</h3>

<p>In the previous two divisions, we started from raw text data, where input key-value pairs were each row
of the text file. In some cases it might be more efficient to go from one subset division method to
another.  Either way, it&#39;s a good thing to know how to do.  In this section we&#39;ll recreate the 
division by state from the division by county.</p>

<p>Let&#39;s start by removing our existing state subsets from the HDFS so there aren&#39;t too many copies
floating around.  We can delete files from the HDFS using the <code>rhdel</code> function.  It only requires
one argument - the directory to be deleted - and it recursively deletes any subdirectories or files.</p>

<p>We can use <code>rhls</code> to see that the division by state is on the HDFS, right where we put it:</p>

<pre><code class="r">rhls(&quot;/ln/tongx/housing&quot;)
</code></pre>

<pre><code>Philip, you should put the output of rhls right here
</code></pre>

<p>We can delete it with the <code>rhdel</code> function:</p>

<pre><code class="r">rhdel(&quot;/ln/tongx/housing/byState&quot;)
</code></pre>

<pre><code>Philip, you should put the output of rhdel right here
</code></pre>

<p>And we can use <code>rhls</code> again to see that we were successful:</p>

<pre><code class="r">rhls(&quot;/ln/tongx/housing&quot;)
</code></pre>

<pre><code>Philip, you should put the new output of rhls right here
</code></pre>

<p>As long as we&#39;re talking about HDFS file management, let&#39;s try out a few more functions.  Suppose
we want to make a copy of the original text file on the HDFS.  We can use the
<code>rhcp</code> function.  It takes two arguments.  The first is the source, or the directory on HDFS we 
want to copy.  The second is the target, or location for the new copy.</p>

<pre><code class="r">rhcp(&quot;/ln/tongx/housing/housing.txt&quot;, &quot;/ln/tongx/housing/tmp/housing.txt&quot;)
rhls(&quot;/ln/tongx/housing/tmp&quot;
</code></pre>

<pre><code>Philip, you should put the output of rhls here
</code></pre>

<p>We can use <code>rhmv</code> to move the text file to a different folder.  This
function also takes two arguments, just like <code>rhcp</code>.</p>

<pre><code class="r">rhmv(&quot;/ln/tongx/housing/tmp/housing.txt&quot;, &quot;/ln/tongx/housing/tmp2/housing.txt&quot;)
rhls(&quot;/ln/tongx/housing/&quot;
</code></pre>

<pre><code>Philip, you should put the output of rhls here
</code></pre>

<p>That&#39;s enough file management.  Now let&#39;s recreate the division by state from the division by 
county, which is still on the HDFS.</p>

<h4>Map</h4>

<pre><code class="r">map5 &lt;- expression({
  lapply(seq_along(map.keys), function(r) {
    key &lt;- attr(map.values[[r]], &quot;state&quot;)
    value &lt;- map.values[[r]]
    value$FIPS &lt;- attr(map.keys[[r]], &quot;FIPS&quot;)
    value$county &lt;- attr(map.keys[[r]], &quot;county&quot;)
    rhcollect(key, value)
  })
})
</code></pre>

<p>The most important part of this map expression is the key we assigned.  Our input key was the length 3
character vector containing FIPS code, county name, and state name.  Our output key is the state
name only.  This ensures that the data frames of all the counties belonging to a single state go to
a single reduce task, so that we can combine them into a single data frame for that state.  Meanwhile,
we also want to make the FIPS code and county name columns of our data frame again.</p>

<h4>Reduce</h4>

<p>We can use the same reduce expression that we used in the first division by state.  But keep in mind
that this time, the intermediate key-value pairs are different.  Before there was one key-value pair
for each line of the text file, but now there is one for each county.</p>

<pre><code class="r">reduce5 &lt;- expression(
  pre = {
    combine &lt;- data.frame()
  },
  reduce = {
    combine &lt;- rbind(combine, do.call(rbind, reduce.values))
  },
  post = {
    rhcollect(reduce.key, combine)
  }
)
</code></pre>

<h4>Execution Function</h4>

<p>Let&#39;s do something new in the <code>rhwatch</code> function.  We&#39;ll set the <code>noeval</code> argument to indicate
that we don&#39;t want to run this job, we just want to package it to be run later.</p>

<pre><code class="r">mr5 &lt;- rhwatch(
  map      = map5,
  reduce   = reduce5,
  input    = rhfmt(&quot;/ln/tongx/housing/byCounty&quot;, type = &quot;sequence&quot;),
  output   = rhfmt(&quot;/ln/tongx/housing/byState&quot;, type = &quot;sequence&quot;),
  mapred   = list( 
    mapred.reduce.tasks = 10
  ),
  readback = FALSE,
  noeval = TRUE
)
</code></pre>

<p>You won&#39;t see any output, because no commands have been sent to Hadoop.  Instead, the packaged
job has been stored in the <code>mr5</code> object.  When we&#39;re ready to run the job, we call <code>rhex()</code>.</p>

<pre><code class="r">byState &lt;- rhex(mr5, async = FALSE)
</code></pre>

<p>The first argument to <code>rhex</code> is the packaged job we just created.  The second argument <code>async</code> 
specifies whether the job should be run asynchronously.  If we set <code>async = FALSE</code> then we&#39;ll see
continuously updated job status information, and we won&#39;t be able to issue any further R commands
until the job completes.  If we set it to <code>TRUE</code>, which is the default, the job will run in the 
background while we continue to interact with our R session.  It&#39;s similar to the <code>\&amp;</code> command in
a Linux shell.</p>

<p>After the job successfully completes, we&#39;ll have state subsets in the specified output folder
on the HDFS, just as before, but created in a different way.</p>

</div>


<div class='tab-pane' id='access-subset-by-state'>
<h3>Access Subset by State</h3>

<p>As we have already
seen in previous examples, our R objects from a mapreduce job have been saved as a sequence file on HDFS.
We also have seen that how to control the number of key-value pairs we want to read by specifying the <code>max</code>
argument in <code>rhread</code>. However, this can only guarantee we read fixed number of key-value pairs. The order
of the key-value pairs or those R objects saved on HDFS is random. What if we would like to only access 
specific key-value pairs by the key without reading all key-value pairs back into the R global environment?
In other words, can we treat our dataset as a queryable database through <code>Rhipe</code> functions? The answer is
absolutely YES!</p>

<p>In order to achieve this purpose, we have to save our output as a map file instead of sequence file on HDFS.
Map file is another type of file can be saved on HDFS besides text and sequence file. For example, we are 
still trying to get subsets for each state based on the subsets for each county. This time we would like to 
be able to access the subset of data for Indiana state only. The code is showed as following:</p>

<pre><code class="r">map5 &lt;- expression({
  lapply(seq_along(map.keys), function(r) {
    key &lt;- attr(map.values[[r]], &quot;state&quot;)
    county &lt;- attr(map.values[[r]], &quot;county&quot;)
    value &lt;- map.values[[r]]
    value$fips &lt;- rep(map.keys[[r]], nrow(value))
    value$county &lt;- rep(county, nrow(value))
    rhcollect(key, value)
  })
})
reduce5 &lt;- expression(
  pre = {
    combine &lt;- data.frame()
  },
  reduce = {
    combine &lt;- rbind(combine, do.call(rbind, reduce.values))
  },
  post = {
    rhcollect(reduce.key, combine)
  }
)
mr5 &lt;- rhwatch(
  map      = map5,
  reduce   = reduce5,
  input    = rhfmt(&quot;/ln/tongx/housing/bycounty&quot;, type = &quot;sequence&quot;),
  output   = rhfmt(&quot;/ln/tongx/housing/countytostate.map&quot;, type = &quot;map&quot;),
  mapred   = list( 
    mapred.reduce.tasks = 10
  ),
  readback = FALSE
)
</code></pre>

<p>It is not hard to notice that the only difference between the code above with the code in the previous
session is that the <code>type</code> in <code>output</code> argument in <code>rhwatch</code> function is changed to be <code>map</code>, which means
we are creating a map file output in this mapreduce job. In this mapreduce job, we are doing exactly same
division of the dataset as we did in sequence file, except that we would like to add indexing by key 
property in the output file. And this is the main difference between map file and sequence file. After
the job is finished, we can access a map file with <code>rhmapfile</code> function.</p>

<pre><code class="r">rst &lt;- rhmapfile(&quot;/ln/tongx/housing/countytostate.map&quot;)
rst
</code></pre>

<pre><code>/ln/tmp/housing/countytostate.map is a MapFile with 10 index files
</code></pre>

<pre><code class="r">class(rst)
</code></pre>

<pre><code>[1] &quot;mapfile&quot;
</code></pre>

<pre><code class="r">object.size(rst)
</code></pre>

<pre><code>264 bytes
</code></pre>

<p>We can see that <code>rst</code> is a MapFile with 10 index files because we specified the reduce tasks number 
to be 10. Its class is <code>mapfile</code>, and size is only 264 bytes, which means we did not read the whole R
object back from HDFS.</p>

<p>Now let us have a deeper look at the output files on HDFS. Recall that &quot;/user/tongx/housing/countytostate&quot;
is the sequence output files, and &quot;/user/tongx/housing/countytostate.map&quot; is the map output files.</p>

<pre><code class="r">rhls(&quot;/ln/tongx/housing/countytostate&quot;)
</code></pre>

<pre><code>   permission owner      group     size          modtime                                         file
1  -rw-r--r-- tongx supergroup        0 2014-09-20 18:02     /ln/tongx/housing/countytostate/_SUCCESS
2  drwxrwxrwx tongx supergroup        0 2014-09-20 18:02        /ln/tongx/housing/countytostate/_logs
3  -rw-r--r-- tongx supergroup 235.1 kb 2014-09-20 18:02 /ln/tongx/housing/countytostate/part-r-00000
4  -rw-r--r-- tongx supergroup   969 kb 2014-09-20 18:02 /ln/tongx/housing/countytostate/part-r-00001
5  -rw-r--r-- tongx supergroup 1.698 mb 2014-09-20 18:02 /ln/tongx/housing/countytostate/part-r-00002
......
</code></pre>

<pre><code class="r">rhls(&quot;/ln/tongx/housing/countytostate.map&quot;)
</code></pre>

<pre><code>   permission owner      group size          modtime                                             file
1  -rw-r--r-- tongx supergroup    0 2014-09-28 13:52     /ln/tongx/housing/countytostate.map/_SUCCESS
2  drwxrwxrwx tongx supergroup    0 2014-09-28 13:51        /ln/tongx/housing/countytostate.map/_logs
3  drwxr-xr-x tongx supergroup    0 2014-09-28 13:52 /ln/tongx/housing/countytostate.map/part-r-00000
4  drwxr-xr-x tongx supergroup    0 2014-09-28 13:52 /ln/tongx/housing/countytostate.map/part-r-00001
5  drwxr-xr-x tongx supergroup    0 2014-09-28 13:52 /ln/tongx/housing/countytostate.map/part-r-00002
......
</code></pre>

<p>We found out that all <code>part-r-</code> files now becomes directory instead of data files. When we go inside of each
<code>part-r-</code> files, we notice that we have two files which are <code>data</code> and <code>index</code>. <code>data</code> file is similar with 
the output in <code>part-r-</code> files we got in sequence output files. We can actually read it by using <code>rhread</code> 
function. The extra <code>index</code> file with size around 200 bytes records all indexing information by key of this 
part file.</p>

<pre><code class="r">rhls(&quot;/ln/tongx/housing/countytostate.map/part-r-00000&quot;)
</code></pre>

<pre><code>  permission owner      group        size          modtime                                                   file
1 -rw-r--r-- tongx supergroup    235.1 kb 2014-09-28 13:52  /ln/tongx/housing/countytostate.map/part-r-00000/data
2 -rw-r--r-- tongx supergroup   209 bytes 2014-09-28 13:52 /ln/tongx/housing/countytostate.map/part-r-00000/index
</code></pre>

<p>Finally, we can access the subset of data frame of Indiana state by:</p>

<pre><code class="r">head(rst[[&quot;IN&quot;]])
</code></pre>

<pre><code>        time  sold      list  selling  fips county
1 2010-11-01    NA  56.70013       NA 18177  Wayne
2 2010-12-01    NA  52.01389       NA 18177  Wayne
3 2011-01-01    NA  49.92504       NA 18177  Wayne
4 2011-02-01    NA  51.91466       NA 18177  Wayne
5 2011-03-01    NA  55.27211       NA 18177  Wayne
6 2011-04-01    NA  56.68348       NA 18177  Wayne
</code></pre>

</div>


<div class='tab-pane' id='plots-by-state'>
<h3>Plots by State</h3>

<p>After previous several sessions, we have seen how to do divide and recombine in <code>Rhipe</code>, how to 
access the subset by key. Another useful and powerful aspect of <code>Rhipe</code> is that we can achieve
parallel plotting on cluster. Visualizing each subset can help us to have a better understanding of
the subsets as well as the whole dataset.</p>

<p>For each subset by state, we would like to know what the number of sold units looks like over time
conditional on county. So we will plot the sold units against time for each state. When we are 
facing a relative small data set in base R, this plotting for each state has to be done either using
a for loop or some other package like <code>plyr</code> in base R. But when we are facing a large data set that
cannot be fit into the memory, previous visualization method will fail easily. </p>

<p>In <code>Rhipe</code>, on the other hand, we can specify multiple tasks, and in each tasks we can plot one or
multiple plots. Later on we will see, in our housing example, we will specify 12 tasks to conduct 
total 49 plots, one for each subset by state. So each task will handle about 4 plots which will 
dramatically decreasing the total time to finish these 49 plots comparing with doing this using for
loop.</p>

<h4>Time Series Plot of Sold Units</h4>

<p>The first plot is the time series plot of the number of sold units conditional on county in each state.
The plotting function we are going to use is in a R package named <code>lattice</code>. Xiaosu will add more words ...</p>

<pre><code class="r">map6 &lt;- expression({
  lapply(seq_along(map.keys), function(r) {
    value &lt;- subset(map.values[[r]], !is.na(as.numeric(units)))
    if(nrow(value) != 0) {
      rhcollect(map.keys[[r]], value)
    }
  })
})
</code></pre>

<p>Xiaosu will add more words here to describe the details about the code.</p>

<pre><code class="r">reduce6 &lt;- expression(
  pre = {
    onestate &lt;- data.frame()
  },
  reduce = {
    onestate &lt;- do.call(rbind, reduce.values)
  },
  post = {
    onestate$date &lt;- as.POSIXct(
      x      = strptime(onestate$date, format = &quot;%Y-%m-%d&quot;), 
      format = &#39;%Y%m%d&#39;, 
      tz     = &quot;&quot;
    )
    trellis.device(pdf, 
      file  = paste(&quot;./tmp/time.vs.unit&quot;, reduce.key, &quot;pdf&quot;, sep=&quot;.&quot;), 
      color = TRUE,
      paper = &quot;legal&quot;
    )
    b &lt;- xyplot(
      log2(as.numeric(units)) ~ date | FIPS,
      data   = onestate,
      layout = c(4, 3),
      cex    = 0.5,
      pch    = 16,
      xlab   = &quot;Time&quot;,
      ylab   = &quot;Log of Number of Sold Units(log base 2)&quot;,
      main   = paste(&quot;Sold Units vs Time in State &quot;, reduce.key, sep=&quot;&quot;)
    )
    print(b)
    dev.off()    
  }
)
</code></pre>

<pre><code class="r">mr6 &lt;- rhwatch(
  map       = map6,
  reduce    = reduce6,
  input     = rhfmt(&quot;/ln/tongx/housing/byState&quot;, type = &quot;sequence&quot;),
  output    = rhfmt(&quot;/ln/tongx/housing/timeplotbystate&quot;, type = &quot;sequence&quot;),
  setup     = expression(
    map = {
    },
    reduce = {
      library(lattice)
    }
  ),
  mapred    = list( 
    mapred.reduce.tasks = 12
  ),
  copyFiles = TRUE,
  readback  = FALSE
)
</code></pre>

<p>Xiaosu will add more words here to talk about how to cope the pdf plots from HDFS to frontend node.</p>

<pre><code class="r">rhls(&quot;/ln/tongx/housing/timeplotbystate&quot;)
</code></pre>

<pre><code>   permission owner      group        size          modtime                                           file
1  -rw-r--r-- tongx supergroup           0 2014-10-02 21:20     /ln/tongx/housing/timeplotbystate/_SUCCESS
2  drwxrwxrwx tongx supergroup           0 2014-10-02 21:20        /ln/tongx/housing/timeplotbystate/_logs
3  drwxr-xr-x tongx supergroup           0 2014-10-02 21:20     /ln/tongx/housing/timeplotbystate/_outputs
4  -rw-r--r-- tongx supergroup    94 bytes 2014-10-02 21:20 /ln/tongx/housing/timeplotbystate/part-r-00000
5  -rw-r--r-- tongx supergroup    94 bytes 2014-10-02 21:20 /ln/tongx/housing/timeplotbystate/part-r-00001
......
</code></pre>

<pre><code class="r">rhls(&quot;/ln/tongx/housing/timeplotbystate/_outputs&quot;)
</code></pre>

<pre><code>   permission owner      group     size          modtime                                                           file
1  -rw-r--r-- tongx supergroup 53.21 kb 2014-10-02 21:20 /ln/tongx/housing/timeplotbystate/_outputs/time.vs.unit.AL.pdf
2  -rw-r--r-- tongx supergroup 136.6 kb 2014-10-02 21:20 /ln/tongx/housing/timeplotbystate/_outputs/time.vs.unit.AR.pdf
3  -rw-r--r-- tongx supergroup 31.86 kb 2014-10-02 21:20 /ln/tongx/housing/timeplotbystate/_outputs/time.vs.unit.AZ.pdf
4  -rw-r--r-- tongx supergroup 150.6 kb 2014-10-02 21:20 /ln/tongx/housing/timeplotbystate/_outputs/time.vs.unit.CA.pdf
5  -rw-r--r-- tongx supergroup 106.7 kb 2014-10-02 21:20 /ln/tongx/housing/timeplotbystate/_outputs/time.vs.unit.CO.pdf
......
</code></pre>

<pre><code class="r">rhget(&quot;/ln/tongx/housing/timeplotbystate/_outputs/time.vs.unit.NC.pdf&quot;, &quot;./&quot;)
rhget(&quot;/ln/tongx/housing/timeplotbystate/_outputs/time.vs.unit.TX.pdf&quot;, &quot;./&quot;)
</code></pre>

<p>The results are here:</p>

<p><a href="./plots/time.vs.unit.NC.pdf">Time series plot of number of units sold for North Carolina State</a>.</p>

<p><a href="./plots/time.vs.unit.TX.pdf">Time series plot of number of units sold for Texas State</a>.</p>

<h4>Time Series Plot of List Price</h4>

<pre><code class="r">map7 &lt;- expression({
  lapply(seq_along(map.keys), function(r) {
    value &lt;- subset(
      map.values[[r]], !is.na(as.numeric(list))
    )
    if(nrow(value) != 0) {
      rhcollect(map.keys[[r]], value)
    }
  })
})
</code></pre>

<pre><code class="r">reduce7 &lt;- expression(
  pre = {
    onestate &lt;- data.frame()
  },
  reduce = {
    onestate &lt;- do.call(rbind, reduce.values)
  },
  post = {
    onestate$date &lt;- as.POSIXct(
      x      = strptime(onestate$date, format = &quot;%Y-%m-%d&quot;), 
      format = &#39;%Y%m%d&#39;, 
      tz     = &quot;&quot;
    )
    trellis.device(pdf, 
      file  = paste(&quot;./tmp/list.vs.time&quot;, reduce.key, &quot;pdf&quot;, sep=&quot;.&quot;), 
      color = TRUE,
      paper = &quot;legal&quot;
    )
    b &lt;- xyplot(
      log2(as.numeric(list)) ~ date | FIPS,
      data   = onestate,
      layout = c(4,3),
      cex    = 0.5,
      pch    = 16,
      scale  = list(
        y = list(relation = &quot;sliced&quot;) 
      ),
      xlab   = &quot;time&quot;,
      ylab   = &quot;Log of list price (log base 2 dollar per square foot)&quot;,
      main   = paste(&quot;List Price vs Time in State &quot;, reduce.key, sep=&quot;&quot;)
    )
    print(b)
    dev.off()    
  }
)
</code></pre>

<pre><code class="r">mr7 &lt;- rhwatch(
  map       = map7,
  reduce    = reduce7,
  input     = rhfmt(&quot;/ln/tongx/housing/byState&quot;, type = &quot;sequence&quot;),
  output    = rhfmt(&quot;/ln/tongx/housing/listplotbystate&quot;, type = &quot;sequence&quot;),
  setup     = expression(
    reduce = {
      library(lattice)
    }
  ),
  mapred    = list( 
    mapred.reduce.tasks = 12
  ),
  copyFiles = TRUE,
  readback  = FALSE
)
</code></pre>

<pre><code class="r">rhls(&quot;/ln/tongx/housing/listplotbystate/_outputs&quot;)
</code></pre>

<pre><code>   permission owner      group     size          modtime                                                           file
1  -rw-r--r-- tongx supergroup 197.6 kb 2014-10-02 22:23 /ln/tongx/housing/listplotbystate/_outputs/list.vs.time.AL.pdf
2  -rw-r--r-- tongx supergroup 213.2 kb 2014-10-02 22:23 /ln/tongx/housing/listplotbystate/_outputs/list.vs.time.AR.pdf
3  -rw-r--r-- tongx supergroup 51.46 kb 2014-10-02 22:23 /ln/tongx/housing/listplotbystate/_outputs/list.vs.time.AZ.pdf
4  -rw-r--r-- tongx supergroup 192.2 kb 2014-10-02 22:23 /ln/tongx/housing/listplotbystate/_outputs/list.vs.time.CA.pdf
......
</code></pre>

<pre><code class="r">rhget(&quot;/ln/tongx/housing/listplotbystate/_outputs/list.vs.time.VA.pdf&quot;, &quot;./&quot;)
rhget(&quot;/ln/tongx/housing/listplotbystate/_outputs/list.vs.time.CA.pdf&quot;, &quot;./&quot;)
</code></pre>

<p><a href="./plots/list.vs.time.CA.pdf">Time series plot of list price for California State</a>.</p>

<p><a href="./plots/list.vs.time.VA.pdf">Time series plot of list price for Virginia State</a>.</p>

</div>


<div class='tab-pane' id='manage-jobs'>
<h3>Manage Jobs</h3>

<p>As you submit more Rhipe jobs, you&#39;ll need to check their status and manage them.  In this short
section, we&#39;ll introduce two new functions to do just that. </p>

<p>First, let&#39;s submit a simple job:</p>

<pre><code class="r">map6 &lt;- expression(
  while(TRUE) {}
)
mr6 &lt;- rhwatch(
  map      = map6,
  input    = rhfmt(&quot;/ln/tongx/housing/housing.txt&quot;, type = &quot;text&quot;),
  output   = rhfmt(&quot;/ln/tongx/housing/tmp&quot;, type = &quot;sequence&quot;),
  mapred   = list(
    mapred.reduce.tasks = 0
  ),
  readback = FALSE,
  noeval   = TRUE
)
badjob &lt;- rhex(mr6, async = TRUE) 
</code></pre>

<p>We&#39;ve set <code>mapred.reduce.tasks</code> to be 0, because this job omits the optional reduce expression.<br>
The map contains an infinite loop, so this job will never complete. The only purpose for this meaningless
map expression is we want the job is keeping running when we try to access its status. But since we&#39;ve run
this job on the backgroup, we can check its status with <code>rhstatus</code>, which takes as an argument
the object returned by <code>rhex</code>. When we said background, it means that the job is running on the 
Hadoop, and we still have the control ability of R session at the same time. </p>

<pre><code class="r">rhstatus(badjob)
</code></pre>

<pre><code>[Tue Sep 30 00:34:20 2014] Name:2014-09-30 00:34:05 Job: job_201405301308_4753  State: RUNNING Duration: 14.673
URL: http://deneb.stat.purdue.edu:50030/jobdetails.jsp?jobid=job_201405301308_4753
             pct numtasks pending running complete killed failed_attempts
map    0.0191228        1       0       1        0      0               0
reduce 0.0000000        0       0       0        0      0               0
       killed_attempts
map                  0
reduce               0
Waiting 5 seconds
</code></pre>

<p>Type <code>ctrl+c</code> to stop the status updates.  Since we know this job won&#39;t ever complete, 
we&#39;ll have to kill it.  Again, use the object returned by <code>rhex</code></p>

<pre><code class="r">rhkill(badjob)
</code></pre>

<p>You can call <code>rhstatus</code> again to see that the job is now in state <code>KILLED</code></p>

</div>


<div class='tab-pane' id='introduction'>
<h3>Introduction</h3>

<p>We&#39;ll demonstrate the computation ability of <code>RHIPE</code> package with a performance test by using logistic
regression. </p>

<p>The basic idea is firstly, we wishto generate an observation sample with a size of <em>N</em>, <em>P</em> number of 
explanatory variables and one response variables. Let&#39;s denote the number of the variables as <em>V</em>, 
which equals to <em>P+1</em>. In summary, we wish to generate a dataframe with <em>N</em> rows and <em>V</em> collumns. 
Secondly, We want to use the logistic regression method to analyze the dataframe. However, when the
data size is quite large, it will be impractical to generate and analyse the whole data set in terms
of the time cosuming and the memory limitation.</p>

<p>Instead, we will use <code>RHIPE</code> package to generate subsets first and then use logistic regression 
method to analyze each subset by R function <code>glm.fit</code>. The subset size is <em>M</em> and the number of 
subsets is <em>R</em>. As a result, \(N = M * R\). We will use R function <code>system.time</code> to measure elapsed 
time which will be explained more in the later session.</p>

<p>In this example, we will show how to do a sing run of the performance test, which means we only pick
one possible value of <em>M</em> and one possible value of <em>V</em> to see how fast it is to go. </p>

<p>We choose \(n = log2(N) = 23\), \(v = log2(V) = 5\), and \(m = log2(M) = 13\) to illustrate one single run.</p>

</div>


<div class='tab-pane' id='generate-datasets'>
<h3>Generate Datasets</h3>

<p>The first step in a D/&amp;R analysis is to choose a division method and create subsets. In this example,
we devide the whole dataset to <em>R</em> sets, which $ log2(R) = 10$. So we will generate 2<sup>10</sup> matrices of 
2<sup>13</sup> rows and 2<sup>5</sup> columns each. </p>

<p>Before we run the map function, we will set up some basic parameters:</p>

<pre><code class="r">n &lt;- 23
m &lt;- 13
v &lt;- 5
p &lt;- 2^v - 1
</code></pre>

<h4>Map</h4>

<pre><code class="r">map1 &lt;- expression({
      for (r in map.values){
        set.seed(r)
        value = matrix(c(rnorm(m*p), sample(c(0,1), m, replace=TRUE)), ncol=p+1)
        rhcollect(r, value)
      }
    })
</code></pre>

<p>The input keys are numeric numbers as 1, 2, ...., <em>R</em>, which log2(R) = log2(N/M) = 10. They are the 
elements of the list object <code>map.keys</code>. We specify the key value as a matrix with 2<sup>13</sup> rows and 
2<sup>5</sup> columns. For each input key-value pair, <code>rhcollect</code> emits an intermediate key-value pair, which 
is the same as the input key-value pair.</p>

<p>The first step is to generate the subsets and we don&#39;t need the <code>reduce</code> function to this point. 
Next, we will write the generated subsets to HDFS.</p>

<h4>Execution Function</h4>

<pre><code class="r">dir.dm  = &quot;/ln/song273/tmp/multi.factor_n23/dm/n23v5m13&quot;
mr &lt;- rhwatch(
  map     = map1,
  input   = c(2^(n-m),12),
  output  = dir.dm,
  jobname = dir.dm,
  mapred  = list(
    mapred.task.timeout=0,
    mapred.reduce.tasks=0),
  parameters = list(m = 2^m, p = p),
  noeval  = TRUE,
  readback = FALSE
  )
t  &lt;- as.numeric(system.time({rhex(mr, async=FALSE)})[3])
</code></pre>

<p>The <code>rhwatch</code> function packages and executes our Rhipe MapReduce job. It consists of  several 
arguments, such as <code>map</code>, <code>input</code>, <code>output</code>, <code>jobname</code>, <code>mapred</code>, <code>parameters</code>,<code>noeval</code> and 
<code>readback</code>, most of which have been introduced in the <code>housing</code> extample. Here we have several new 
arguments, like <code>parameters</code>, <code>jobname</code> and <code>noeval</code>. The argument <code>parameters</code> is a named list with 
parameters to be passed to a mapreduce job which in our case, <code>map1</code> is an R expression, and we need
to pass the values of <code>m</code> and <code>p</code> to the argument <code>map</code> in <code>rhwatch</code>. The argument <code>jobname</code> is the 
name of the job, which is visible on the Jobtracker website. If not provided, Hadoop MapReduce uses
the default name job_date_time_number e.g. job_201007281701_0274. If the argument <code>noeval</code> is set to
be &#39;TRUE&#39;, then the Hadoop MapReduce job will not run, just return the job object. You can view <code>rhwatch</code>
function for more details.</p>

<p>Instead, the <code>rhex</code> function will submit the MapReduce job to the Hadoop MapReduce framework. The 
following is the output of the last R command:</p>

<pre><code></code></pre>

<p>Our  datasets have been created and are saved on HDFS now. We can see more details about the files 
on HDFS by <code>rhls()</code>.</p>

<pre><code class="r">rhls(&quot;/ln/song273/tmp/multi.factor_n23/dm/n23v5m13&quot;)
</code></pre>

<pre><code>   permission   owner      group     size          modtime                                                      file
1  -rw-r--r-- song273 supergroup        0 2014-10-02 22:36     /ln/song273/tmp/multi.factor_n23/dm/n23v5m13/_SUCCESS
2  drwxrwxrwt song273 supergroup        0 2014-10-02 22:36        /ln/song273/tmp/multi.factor_n23/dm/n23v5m13/_logs
3  -rw-r--r-- song273 supergroup   178 mb 2014-10-02 22:36 /ln/song273/tmp/multi.factor_n23/dm/n23v5m13/part-m-00000
4  -rw-r--r-- song273 supergroup   170 mb 2014-10-02 22:36 /ln/song273/tmp/multi.factor_n23/dm/n23v5m13/part-m-00001
5  -rw-r--r-- song273 supergroup   170 mb 2014-10-02 22:36 /ln/song273/tmp/multi.factor_n23/dm/n23v5m13/part-m-00002
6  -rw-r--r-- song273 supergroup   170 mb 2014-10-02 22:36 /ln/song273/tmp/multi.factor_n23/dm/n23v5m13/part-m-00003
7  -rw-r--r-- song273 supergroup   170 mb 2014-10-02 22:36 /ln/song273/tmp/multi.factor_n23/dm/n23v5m13/part-m-00004
8  -rw-r--r-- song273 supergroup   170 mb 2014-10-02 22:36 /ln/song273/tmp/multi.factor_n23/dm/n23v5m13/part-m-00005
9  -rw-r--r-- song273 supergroup   170 mb 2014-10-02 22:36 /ln/song273/tmp/multi.factor_n23/dm/n23v5m13/part-m-00006
10 -rw-r--r-- song273 supergroup   170 mb 2014-10-02 22:36 /ln/song273/tmp/multi.factor_n23/dm/n23v5m13/part-m-00007
11 -rw-r--r-- song273 supergroup   170 mb 2014-10-02 22:36 /ln/song273/tmp/multi.factor_n23/dm/n23v5m13/part-m-00008
12 -rw-r--r-- song273 supergroup   170 mb 2014-10-02 22:36 /ln/song273/tmp/multi.factor_n23/dm/n23v5m13/part-m-00009
13 -rw-r--r-- song273 supergroup   170 mb 2014-10-02 22:36 /ln/song273/tmp/multi.factor_n23/dm/n23v5m13/part-m-00010
14 -rw-r--r-- song273 supergroup   170 mb 2014-10-02 22:36 /ln/song273/tmp/multi.factor_n23/dm/n23v5m13/part-m-00011
</code></pre>

<p>Comments: give description of the first two files. As we can see from the output, the size for one 
output file is 170 mb. It would be impractical to read the whole data sets from HDFS to our R memory.
We can only read one file from HDFS to the R memory by specify the argument <code>max</code> of RHIPE function 
<code>rhread</code> equals to 1. Then we will use R function <code>str</code> to explore the structure.</p>

<pre><code class="r">a  = rhread(&quot;/ln/song273/tmp/multi.factor_n23/dm/n23v5m13&quot;,max = 1)
str(a)
</code></pre>

<pre><code>List of 1
 $ :List of 2
  ..$ : num 936
  ..$ : num [1:8192, 1:32] 0.0803 -1.5415 -1.1148 1.8888 1.9351 ...
</code></pre>

<p>So &#39;a&#39; is a list with two elements key and value. The value is a 8192 * 32 matrix.</p>

</div>


<div class='tab-pane' id='elapsed-time-measurement'>
<h3>Elapsed Time Measurement</h3>

<p>There are two types of elapsed-time computation. The subsets are stored on the HDFS as R objects. 
The first computation type is <strong>O</strong>, the elapsed time to read the subsets from the HDFS and make 
them available to <code>glm.fit</code> in memory as an R objects. The other type, <strong>L</strong>, starts when <strong>O</strong> ends
and it consists of <code>glm.fit</code> computations on the subsets by <strong>map</strong>, plus <strong>reduce</strong> gathering the 
subset estimates and computing the means. However, we cannot measure <strong>L</strong> directly. So we measure 
<strong>O</strong> in one run and <strong>T = O + L</strong> in another.</p>

<h4>The First Kind Of Elapsed Time -- <strong>O</strong></h4>

<h4>The Second Kind of Elapsed Time -- <strong>T</strong></h4>

<h4>Performance Test Results</h4>

<p>one The cluster which we are going to test the computation performance is the Deneb/Mimosa Cluster.The 
Deneb cluster is maintained by system administrators at the Department of Statistics Purdue University.
It is used for teaching distributed parallel computing for the analysis of large complex data.
There are two nodes of this cluster, one runs NameNode and the other runs JobTracker. Both run Hadoop
DataNode and TaskTracker, and each has 8 cores, 32 GB memory, 2 TB diske. Collectively, the cluster 
has 16 cores, 64 GB memory and 4 TB disk. In general, we will save 4 cores for the general management
or storage, and the other 12 cores will be used for map/reduce jobs.</p>

</div>


<div class='tab-pane' id='r-code----set-up'>
<h3>R code -- Set Up</h3>

<pre><code class="r"># experiment name
name &lt;- &quot;multi.factor_n21&quot;
# top level directory for experiment on HDFS
dir = &quot;/ln/song273/tmp&quot;
# directory for this experiment on HDFS
dir.exp = file.path(dir, name)
# directory for local file system
dir.local = &quot;/home/median/u41/song273/timetest/&quot;
# break time in seconds between jobs
sleep = 10
# number of replicate runs
run.vec = 1:3

## subset factors 
# log2 number of observations
n = 21
# number of predictor variables
p.vec = 2^(4:6) - 1
# log2 number of observations per subset
m.vec = seq(8, 16, by=1)
</code></pre>

</div>


<div class='tab-pane' id='r-code----generate-dataset'>
<h3>R code -- Generate Dataset</h3>

<pre><code class="r">for (m in m.vec) {
  for (p in p.vec) {
    dir.dm = paste(dir.exp,&quot;/dm/&quot;,&#39;n&#39;,n,&#39;p&#39;,p,&#39;m&#39;,m, sep=&quot;&quot;)

    dm = list()
    dm$map = expression({
      for (r in map.values){
        set.seed(r)
        value = matrix(c(rnorm(m*p), sample(c(0,1), m, replace=TRUE)), ncol=p+1)
        rhcollect(r, value) # key is subset id
      }
    })
    dm$input = c(2^(n-m), 12)
    dm$output = dir.dm
    dm$jobname = dm$output
    dm$mapred = list( 
      mapred.task.timeout=0
      , mapred.reduce.tasks=0 
    )
    dm$parameters = list(m=2^m, p=p)
    dm$noeval = TRUE
    dm.mr = do.call(&#39;rhwatch&#39;, dm)
    t = as.numeric(system.time({rhex(dm.mr, async=FALSE)})[3])
    Sys.sleep(time=sleep)
}}


Sys.sleep(time=sleep*10)
</code></pre>

</div>


<div class='tab-pane' id='r-code----timing'>
<h3>R code -- Timing</h3>

<pre><code class="r">## initialize timing
timing = list()

  ## timing for O
  compute = &quot;O&quot;

      dir.dm = paste(dir.exp,&quot;/dm/&quot;,&#39;n&#39;,n,&#39;p&#39;,p,&quot;m&quot;,m, sep=&quot;&quot;)
      dir.nf = paste(dir.exp,&quot;/nf/&quot;,&quot;run&quot;,run,&#39;n&#39;,n,&#39;p&#39;,p,&quot;m&quot;,m, sep=&quot;&quot;)

      nf = list()
      nf$map = expression({})
      nf$mapred = list(
        mapred.reduce.tasks=1
        , rhipe_map_buff_size=2^15
      )
      nf$parameters = list(p=p)
      nf$input = dir.dm
      nf$output = dir.nf
      nf$jobname = nf$output
      nf$noeval = TRUE
      nf.mr = do.call(&#39;rhwatch&#39;, nf)
      t = as.numeric(system.time({rhex(nf.mr, async=FALSE)})[3])
      timing[[length(timing)+1]] = list(compute=compute, n=n, p=p, m=m, run=run, t=t)

      Sys.sleep(time=sleep)
    # end of loop of m and p


  ## timing for T
  compute = &quot;T&quot;
      dir.dm = paste(dir.exp,&quot;/dm/&quot;,&#39;n&#39;,n,&#39;p&#39;,p,&quot;m&quot;,m, sep=&quot;&quot;)
      dir.gf = paste(dir.exp,&quot;/gf/&quot;,&quot;run&quot;,run,&#39;n&#39;,n,&#39;p&#39;,p,&quot;m&quot;,m, sep=&quot;&quot;)

      gf = list()
      gf$map = expression({
        for (v in map.values) {
          value = glm.fit(v[,1:p],v[,p+1],family=binomial())$coef
          rhcollect(1, value)
        }
      })
      gf$reduce = expression(
        pre = { 
          v = rep(0,p) 
          nsub = 0
        },
        reduce = { 
          v = v + colSums(matrix(unlist(reduce.values), ncol=p, byrow=TRUE)) 
          nsub = nsub + length(reduce.values)
        },
        post = { rhcollect(reduce.key, v/nsub) }
      )
      gf$mapred = list(
        mapred.reduce.tasks=1
        , rhipe_map_buff_size=2^15
      )
      gf$parameters = list(p=p)
      gf$input = dir.dm
      gf$output = dir.gf
      gf$jobname = gf$output
      gf$noeval = TRUE
      gf.mr = do.call(&#39;rhwatch&#39;, gf)
      t = as.numeric(system.time({rhex(gf.mr, async=FALSE)})[3])
      timing[[length(timing)+1]] = list(compute=compute, n=n, p=p, m=m, run=run, t=t)


</code></pre>

</div>


<div class='tab-pane' id='r-code----save-the-results'>
<h3>R code -- Save the Results</h3>

<pre><code>
```r
timing = ldply(timing, as.data.frame)
rhsave(timing, file=paste(dir, &quot;/save/&quot;, name, &quot;.RData&quot;, sep=&quot;&quot;))
save(timing, file=paste(dir.local, name, &quot;.RData&quot;, sep=&quot;&quot;))
</code></pre>

</div>


<div class='tab-pane' id='results'>
<h3>Results</h3>

<pre><code class="r">head(timing)
</code></pre>

<pre><code>  compute  n  p m run      t
1       O 21 15 8   1 20.644
2       O 21 31 8   1 19.601
3       O 21 63 8   1 21.530
4       O 21 15 9   1 19.581
5       O 21 31 9   1 19.521
6       O 21 63 9   1 21.493
</code></pre>

</div>


<div class='tab-pane' id='visualize-the-results'>
<h3>Visualize the Results</h3>

<p><img src="./plots/08.timing.pdf" alt="alt text"></p>

</div>

   
   <ul class="pager">
      <li><a href="#" id="previous">&larr; Previous</a></li> 
      <li><a href="#" id="next">Next &rarr;</a></li> 
   </ul>
</div>


</div>
</div>

<hr>

<div class="footer">
   <p>&copy; , 2014</p>
</div>
</div> <!-- /container -->

<script src="assets/jquery/jquery.js"></script>
<script type='text/javascript' src='assets/custom/custom.js'></script>
<script src="assets/bootstrap/js/bootstrap.js"></script>
<script src="assets/custom/jquery.ba-hashchange.min.js"></script>
<script src="assets/custom/nav.js"></script>

</body>
</html>