## Tessera Cluster Logistic Performance Test ##

### Introduction ###

We'll demonstrate the computation ability of `RHIPE` package with a performance test by using logistic
regression. 

The basic idea is firstly, we wishto generate an observation sample with a size of *N*, *P* number of 
explanatory variables and one response variables. Let's denote the number of the variables as *V*, 
which equals to *P+1*. In summary, we wish to generate a dataframe with *N* rows and *V* collumns. 
Secondly, We want to use the logistic regression method to analyze the dataframe. However, when the
data size is quite large, it will be impractical to generate and analyse the whole data set in terms
of the time cosuming and the memory limitation.

Instead, we will use `RHIPE` package to generate subsets first and then use logistic regression 
method to analyze each subset by R function `glm.fit`. The subset size is *M* and the number of 
subsets is *R*. As a result, $N = M * R$. We will use R function `system.time` to measure elapsed 
time which will be explained more in the later session.

In this example, we will show how to do a sing run of the performance test, which means we only pick
one possible value of *M* and one possible value of *V* to see how fast it is to go. 

We choose $n = log2(N) = 23$, $v = log2(V) = 5$, and $m = log2(M) = 13$ to illustrate one single run.

### Generate Datasets ###

The first step in a D/&R analysis is to choose a division method and create subsets. In this example,
we devide the whole dataset to *R* sets, which $ log2(R) = 10$. So we will generate 2^10 matrices of 
2^13 rows and 2^5 columns each. 

Before we run the map function, we will set up some basic parameters:

```{r,eval=FALSE,tidy=FALSE}
n <- 23
m <- 13
v <- 5
p <- 2^v - 1
```

#### Map ####

```{r, eval=FALSE,tidy=FALSE}
map1 <- expression({
      for (r in map.values){
        set.seed(r)
        value = matrix(c(rnorm(m*p), sample(c(0,1), m, replace=TRUE)), ncol=p+1)
        rhcollect(r, value)
      }
    })
```
 
The input keys are numeric numbers as 1, 2, ...., *R*, which log2(R) = log2(N/M) = 10. They are the 
elements of the list object `map.keys`. We specify the key value as a matrix with 2^13 rows and 
2^5 columns. For each input key-value pair, `rhcollect` emits an intermediate key-value pair, which 
is the same as the input key-value pair.

The first step is to generate the subsets and we don't need the `reduce` function to this point. 
Next, we will write the generated subsets to HDFS.

#### Execution Function ####

```{r eval=FALSE, tidy=FALSE}
dir.dm  = "/ln/song273/tmp/multi.factor_n23/dm/n23v5m13"
mr <- rhwatch(
  map     = map1,
  input   = c(2^(n-m),12),
  output  = dir.dm,
  jobname = dir.dm,
  mapred  = list(
    mapred.task.timeout=0,
    mapred.reduce.tasks=0),
  parameters = list(m = 2^m, p = p),
  noeval  = TRUE,
  readback = FALSE
  )
t  <- as.numeric(system.time({rhex(mr, async=FALSE)})[3])

```

Our output is saved on Before we move to the elased time measurement part, we can take a look at the files we created. We 
can use 

a  = rhread("/ln/song273/tmp/multi.factor_n23/dm/n23v5m13",max = 1)

> str(a)
List of 1
 $ :List of 2
  ..$ : num 936
  ..$ : num [1:8192, 1:32] 0.0803 -1.5415 -1.1148 1.8888 1.9351 ...


### Elapsed Time Measurement ###


There are two types of elapsed-time computation. The subsets are stored on the HDFS as R objects. 
The first computation type is **O**, the elapsed time to read the subsets from the HDFS and make 
them available to `glm.fit` in memory as an R objects. The other type, **L**, starts when **O** ends
and it consists of `glm.fit` computations on the subsets by **map**, plus **reduce** gathering the 
subset estimates and computing the means. However, we cannot measure **L** directly. So we measure 
**O** in one run and **T = O + L** in another.

#### The First Kind Of Elapsed Time -- **O** #####

#### The Second Kind of Elapsed Time -- **T** ####

#### Performance Test Results ####

one The cluster which we are going to test the computation performance is the Deneb/Mimosa Cluster.The 
Deneb cluster is maintained by system administrators at the Department of Statistics Purdue University.
It is used for teaching distributed parallel computing for the analysis of large complex data.
There are two nodes of this cluster, one runs NameNode and the other runs JobTracker. Both run Hadoop
DataNode and TaskTracker, and each has 8 cores, 32 GB memory, 2 TB diske. Collectively, the cluster 
has 16 cores, 64 GB memory and 4 TB disk. In general, we will save 4 cores for the general management
or storage, and the other 12 cores will be used for map/reduce jobs.


### Data Structure ###
```{r createtable, results='asis', echo=FALSE}
cat("Variables | Description |Values ","--- | --- |---", sep="\n")
x = c("N","observation sample size","2^21 , 2^23 , 2^25 , 2^27 ","V","Factor--Number of variables","2^4 , 2^5 , 2^6","M","Factor--Number of observations per subset","2^8 , 2^9 , 2^10 , ..., 2^16","O","Response variable--first type of elapsed time "," ","T","Response varibale--whole elapsed time"," ")
x = matrix(x,ncol=3,byrow=TRUE)
cat(apply(x,1,function(X) paste(X,collapse=" | ")),sep = "\n")
```

we will using the following notations in this performance experiment. 

```{r createtable1, results='asis', echo=FALSE}
cat("Variables | Description |Values ","--- | --- |---", sep="\n")
y = c("n","log2(N)","21 , 23 , 25 , 27 ","p","V - 1: Number of explanatory variables","15 , 31 , 63","m","log2(M): log2 number of observations per subset","8 , 9 , 10 , ..., 16")
y = matrix(y,ncol=3,byrow=TRUE)
cat(apply(y,1,function(X) paste(X,collapse=" | ")),sep = "\n")
```

### Experimental Design ###

### R code -- Set Up ###
```{r, eval=FALSE}


# experiment name
name <- "multi.factor_n21"
# top level directory for experiment on HDFS
dir = "/ln/song273/tmp"
# directory for this experiment on HDFS
dir.exp = file.path(dir, name)
# directory for local file system
dir.local = "/home/median/u41/song273/timetest/"
# break time in seconds between jobs
sleep = 10
# number of replicate runs
run.vec = 1:3

## subset factors 
# log2 number of observations
n = 21
# number of predictor variables
p.vec = 2^(4:6) - 1
# log2 number of observations per subset
m.vec = seq(8, 16, by=1)
```
### R code -- Generate Dataset ###
```{r,eval=FALSE}
for (m in m.vec) {
  for (p in p.vec) {
    dir.dm = paste(dir.exp,"/dm/",'n',n,'p',p,'m',m, sep="")
    
    dm = list()
    dm$map = expression({
      for (r in map.values){
        set.seed(r)
        value = matrix(c(rnorm(m*p), sample(c(0,1), m, replace=TRUE)), ncol=p+1)
        rhcollect(r, value) # key is subset id
      }
    })
    dm$input = c(2^(n-m), 12)
    dm$output = dir.dm
    dm$jobname = dm$output
    dm$mapred = list( 
      mapred.task.timeout=0
      , mapred.reduce.tasks=0 
    )
    dm$parameters = list(m=2^m, p=p)
    dm$noeval = TRUE
    dm.mr = do.call('rhwatch', dm)
    t = as.numeric(system.time({rhex(dm.mr, async=FALSE)})[3])
    Sys.sleep(time=sleep)
}}


Sys.sleep(time=sleep*10)
```

### R code -- Timing ###
```{r,eval= FALSE}
## initialize timing
timing = list()

for (run in run.vec) {
  
  ## timing for O
  compute = "O"
  for (m in m.vec) {
    for (p in p.vec) {
      dir.dm = paste(dir.exp,"/dm/",'n',n,'p',p,"m",m, sep="")
      dir.nf = paste(dir.exp,"/nf/","run",run,'n',n,'p',p,"m",m, sep="")
      
      nf = list()
      nf$map = expression({})
      nf$mapred = list(
        mapred.reduce.tasks=1
        , rhipe_map_buff_size=2^15
      )
      nf$parameters = list(p=p)
      nf$input = dir.dm
      nf$output = dir.nf
      nf$jobname = nf$output
      nf$noeval = TRUE
      nf.mr = do.call('rhwatch', nf)
      t = as.numeric(system.time({rhex(nf.mr, async=FALSE)})[3])
      timing[[length(timing)+1]] = list(compute=compute, n=n, p=p, m=m, run=run, t=t)
      
      Sys.sleep(time=sleep)
    }}  # end of loop of m and p
  
  
  ## timing for T
  compute = "T"
  for (m in m.vec) {
    for (p in p.vec) {
      dir.dm = paste(dir.exp,"/dm/",'n',n,'p',p,"m",m, sep="")
      dir.gf = paste(dir.exp,"/gf/","run",run,'n',n,'p',p,"m",m, sep="")
      
      gf = list()
      gf$map = expression({
        for (v in map.values) {
          value = glm.fit(v[,1:p],v[,p+1],family=binomial())$coef
          rhcollect(1, value)
        }
      })
      gf$reduce = expression(
        pre = { 
          v = rep(0,p) 
          nsub = 0
        },
        reduce = { 
          v = v + colSums(matrix(unlist(reduce.values), ncol=p, byrow=TRUE)) 
          nsub = nsub + length(reduce.values)
        },
        post = { rhcollect(reduce.key, v/nsub) }
      )
      gf$mapred = list(
        mapred.reduce.tasks=1
        , rhipe_map_buff_size=2^15
      )
      gf$parameters = list(p=p)
      gf$input = dir.dm
      gf$output = dir.gf
      gf$jobname = gf$output
      gf$noeval = TRUE
      gf.mr = do.call('rhwatch', gf)
      t = as.numeric(system.time({rhex(gf.mr, async=FALSE)})[3])
      timing[[length(timing)+1]] = list(compute=compute, n=n, p=p, m=m, run=run, t=t)
      
      Sys.sleep(time=sleep)
    }}  # end of loop of m and p
  
}   # end of loop of run
```

### R code -- Save the Results ###


```{r,eval=FALSE}
timing = ldply(timing, as.data.frame)
rhsave(timing, file=paste(dir, "/save/", name, ".RData", sep=""))
save(timing, file=paste(dir.local, name, ".RData", sep=""))

```

### Results ###

```{r,eval=FALSE}
head(timing)
```
```
  compute  n  p m run      t
1       O 21 15 8   1 20.644
2       O 21 31 8   1 19.601
3       O 21 63 8   1 21.530
4       O 21 15 9   1 19.581
5       O 21 31 9   1 19.521
6       O 21 63 9   1 21.493
```

### Visualize the Results ###

![alt text](./plots/08.timing.pdf)
