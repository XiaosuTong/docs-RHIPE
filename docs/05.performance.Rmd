## Tessera Cluster Logistic Performance Test ##

### Introduction ###

We'll demonstrate the computation ability of `RHIPE` package with a performance test by using logistic
regression. 

The basic idea is firstly, we wishto generate an observation sample with a size of *N*, *P* number of 
explanatory variables and one response variables. Let's denote the number of the variables as *V*, 
which equals to *P+1*. In summary, we wish to generate a dataframe with *N* rows and *V* collumns. 
Secondly, We want to use the logistic regression method to analyze the dataframe. However, when the
data size is quite large, it will be impractical to generate and analyse the whole data set in terms
of the time cosuming and the memory limitation.

Instead, we will use `RHIPE` package to generate subsets first and then use logistic regression 
method to analyze each subset by R function `glm.fit`. The subset size is *M* and the number of 
subsets is *R*. As a result, $N = M * R$. We will use R function `system.time` to measure elapsed 
time which will be explained more in the later session.

In this example, we will show how to do a sing run of the performance test, which means we only pick
one possible value of *M* and one possible value of *V* to see how fast it is to go. 

We choose $n = log2(N) = 23$, $v = log2(V) = 5$, and $m = log2(M) = 13$ to illustrate one single run.

### Generate Datasets ###

The first step in a D/&R analysis is to choose a division method and create subsets. In this example,
we devide the whole dataset to *R* sets, which $ log2(R) = 10$. So we will generate 2^10 matrices of 
2^13 rows and 2^5 columns each. 

Before we run the map function, we will set up some basic parameters:

```{r,eval=FALSE,tidy=FALSE}
n <- 23
m <- 13
v <- 5
p <- 2^v - 1
```

#### Map ####

```{r, eval=FALSE,tidy=FALSE}
map1 <- expression({
      for (r in map.values){
        set.seed(r)
        value = matrix(c(rnorm(m*p), sample(c(0,1), m, replace=TRUE)), ncol=p+1)
        rhcollect(r, value)
      }
    })
```
 
The input keys are numeric numbers as 1, 2, ...., *R*, which log2(R) = log2(N/M) = 10. They are the 
elements of the list object `map.keys`. We specify the key value as a matrix with 2^13 rows and 
2^5 columns. For each input key-value pair, `rhcollect` emits an intermediate key-value pair, which 
is the same as the input key-value pair.

The first step is to generate the subsets and we don't need the `reduce` function to this point. 
Next, we will write the generated subsets to HDFS.

#### Execution Function ####

```{r eval=FALSE, tidy=FALSE}
dir.dm  = "/ln/song273/tmp/multi.factor_n23/dm/n23v5m13"
mr <- rhwatch(
  map     = map1,
  input   = c(2^(n-m),12),
  output  = dir.dm,
  jobname = dir.dm,
  mapred  = list(
    mapred.task.timeout=0,
    mapred.reduce.tasks=0),
  parameters = list(m = 2^m, p = p),
  noeval  = TRUE,
  readback = FALSE
  )
t  <- as.numeric(system.time({rhex(mr, async=FALSE)})[3])

```

The `rhwatch` function packages and executes our Rhipe MapReduce job. It consists of  several 
arguments, such as `map`, `input`, `output`, `jobname`, `mapred`, `parameters`,`noeval` and 
`readback`, most of which have been introduced in the `housing` extample. Here we have several new 
arguments, like `parameters`, `jobname` and `noeval`. The argument `parameters` is a named list with 
parameters to be passed to a mapreduce job which in our case, `map1` is an R expression, and we need
to pass the values of `m` and `p` to the argument `map` in `rhwatch`. The argument `jobname` is the 
name of the job, which is visible on the Jobtracker website. If not provided, Hadoop MapReduce uses
the default name job_date_time_number e.g. job_201007281701_0274. If the argument `noeval` is set to
be 'TRUE', then the Hadoop MapReduce job will not run, just return the job object. You can view `rhwatch`
function for more details.

Instead, the `rhex` function will submit the MapReduce job to the Hadoop MapReduce framework. The 
following is the output of the last R command:

```
```

Our  datasets have been created and are saved on HDFS now. We can see more details about the files 
on HDFS by `rhls()`.

```{r eval=FALSE, tidy=FALSE}
rhls("/ln/song273/tmp/multi.factor_n23/dm/n23v5m13")
```

```
   permission   owner      group     size          modtime                                                      file
1  -rw-r--r-- song273 supergroup        0 2014-10-02 22:36     /ln/song273/tmp/multi.factor_n23/dm/n23v5m13/_SUCCESS
2  drwxrwxrwt song273 supergroup        0 2014-10-02 22:36        /ln/song273/tmp/multi.factor_n23/dm/n23v5m13/_logs
3  -rw-r--r-- song273 supergroup   178 mb 2014-10-02 22:36 /ln/song273/tmp/multi.factor_n23/dm/n23v5m13/part-m-00000
4  -rw-r--r-- song273 supergroup   170 mb 2014-10-02 22:36 /ln/song273/tmp/multi.factor_n23/dm/n23v5m13/part-m-00001
5  -rw-r--r-- song273 supergroup   170 mb 2014-10-02 22:36 /ln/song273/tmp/multi.factor_n23/dm/n23v5m13/part-m-00002
6  -rw-r--r-- song273 supergroup   170 mb 2014-10-02 22:36 /ln/song273/tmp/multi.factor_n23/dm/n23v5m13/part-m-00003
7  -rw-r--r-- song273 supergroup   170 mb 2014-10-02 22:36 /ln/song273/tmp/multi.factor_n23/dm/n23v5m13/part-m-00004
8  -rw-r--r-- song273 supergroup   170 mb 2014-10-02 22:36 /ln/song273/tmp/multi.factor_n23/dm/n23v5m13/part-m-00005
9  -rw-r--r-- song273 supergroup   170 mb 2014-10-02 22:36 /ln/song273/tmp/multi.factor_n23/dm/n23v5m13/part-m-00006
10 -rw-r--r-- song273 supergroup   170 mb 2014-10-02 22:36 /ln/song273/tmp/multi.factor_n23/dm/n23v5m13/part-m-00007
11 -rw-r--r-- song273 supergroup   170 mb 2014-10-02 22:36 /ln/song273/tmp/multi.factor_n23/dm/n23v5m13/part-m-00008
12 -rw-r--r-- song273 supergroup   170 mb 2014-10-02 22:36 /ln/song273/tmp/multi.factor_n23/dm/n23v5m13/part-m-00009
13 -rw-r--r-- song273 supergroup   170 mb 2014-10-02 22:36 /ln/song273/tmp/multi.factor_n23/dm/n23v5m13/part-m-00010
14 -rw-r--r-- song273 supergroup   170 mb 2014-10-02 22:36 /ln/song273/tmp/multi.factor_n23/dm/n23v5m13/part-m-00011
```
Comments: give description of the first two files. As we can see from the output, the size for one 
output file is 170 mb. It would be impractical to read the whole data sets from HDFS to our R memory.
We can only read one file from HDFS to the R memory by specify the argument `max` of RHIPE function 
`rhread` equals to 1. Then we will use R function `str` to explore the structure.

```{r eval=FALSE, tidy=FALSE}
a  = rhread("/ln/song273/tmp/multi.factor_n23/dm/n23v5m13",max = 1)
str(a)
```

```
List of 1
 $ :List of 2
  ..$ : num 936
  ..$ : num [1:8192, 1:32] 0.0803 -1.5415 -1.1148 1.8888 1.9351 ...
```
So 'a' is a list with two elements key and value. The value is a 8192 * 32 matrix.


### Elapsed Time Measurement ###

There are two types of elapsed-time computation. The subsets are stored on the HDFS as R objects. 
The first computation type is **O**, the elapsed time to read the subsets from the HDFS and make 
them available to `glm.fit` in memory as an R objects. The other type, **L**, starts when **O** ends
and it consists of `glm.fit` computations on the subsets by **map**, plus **reduce** gathering the 
subset estimates and computing the means. However, we cannot measure **L** directly. So we measure 
**O** in one run and **T = O + L** in another.

#### The First Kind Of Elapsed Time -- **O** ####

First, we will initialize a list called `timing` to store these two different kinds of elapsed time
and create a numeric object `compute` to lable **O** and **T** seperately.

```{r eval=FALSE, tidy=FALSE}
timing <- list()
compute = "O"
```

##### Map #####

```{r eval=FALSE, tidy=FALSE}
map2 <- expression({})
```

It is interesting to notice that `map2` is an expression with no command. That's because we have 
already created the data, we don't need new key-value pairs in the reading part. 


#### The Second Kind of Elapsed Time -- **T** ####

#### Performance Test Results ####

one The cluster which we are going to test the computation performance is the Deneb/Mimosa Cluster.The 
Deneb cluster is maintained by system administrators at the Department of Statistics Purdue University.
It is used for teaching distributed parallel computing for the analysis of large complex data.
There are two nodes of this cluster, one runs NameNode and the other runs JobTracker. Both run Hadoop
DataNode and TaskTracker, and each has 8 cores, 32 GB memory, 2 TB diske. Collectively, the cluster 
has 16 cores, 64 GB memory and 4 TB disk. In general, we will save 4 cores for the general management
or storage, and the other 12 cores will be used for map/reduce jobs.

