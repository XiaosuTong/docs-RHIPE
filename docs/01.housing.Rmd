# RHIPE Tutorial #

## Housing Data ##

### Introduction ###

We'll demonstrate the `RHIPE` package with an analysis of housing data.
The housing data set contains monthly median list and selling prices per square foot and number 
of units sold for 2883 counties in 49 states from October 2008 to March 2014, harvested from 
Quandl's Zillow Housing Data.

Here is how raw text file looks like:
```
06001,Alameda,CA,2008-10-01,NA,307.97872340425,325.8118
06001,Alameda,CA,2008-11-01,NA,299.16666666667,NA
06001,Alameda,CA,2008-11-01,NA,NA,318.115
06001,Alameda,CA,2008-12-01,NA,289.88149498633,305.7878
06001,Alameda,CA,2009-01-01,NA,288.5,291.5977
```

There are 224,369 rows and 7 fields. Each field is separated by a comma, and there is no header row.
The meaning of each field is as follows:

- **fips**: FIPS county code
- **county**: county name
- **state**: state abbreviation
- **time**: time in "YYYY-MM-DD" format.  Each month's values are placed at the first day of the month
- **sold**: number of units sold
- **list**: monthly median list price dollar per square foot
- **selling**: monthly median selling price dollar per square foot
 
The total size of the data text file is around 12MB. The raw data are available as named
`housing.txt` in our Tesseradata Github repository of the `RHIPE` documentation 
[here](http://raw.githubusercontent.com/xiaosutong/docs-RHIPE/gh-pages/housing.txt).   

Note:

A housing unit is a house, an apartment, a mobile home, a group of rooms, or a single room that is
occupied (or if vacant, is intended for occupancy) as separate living quarters. Separate living 
quarters are those in which the occupants live and eat separately from any other persons in the 
building and which have direct access from the outside of the building or through a common hall.

### Write Text to HDFS ###

Now we ssh to the frontend of the cluster or any node of the cluster where we will start
the initial R session. When we start the initial R, there is a working directory of the R to which the .RData
and .Rhistory is saved. This working directory is locating on the frontend or the node we are sshing.
We will treat this working directory as our current local working directory.
We can get where this working directory is by using `getwd` function in R:

```{r eval=FALSE, tidy=FALSE}
getwd()
```
```
[1] "/home/tongx"
```
You should have your own working directory wherever is more convenient for yourself.
We will download the text data file to this local working directory by R command:

```{r eval=FALSE, tidy=FALSE}
system("wget https://raw.githubusercontent.com/XiaosuTong/docs-RHIPE/gh-pages/housing.txt /home/tongx")
```

We can check if text file is available now in our local working directory:

```{r eval=FALSE, tidy=FALSE}
list.files("/home/tongx")
```
```
[1] "housing.txt" "Projects"    "Rhipe"       "R_LIBS"      "R_LIBS.new" 
[6] "R_packages"
```

First, let's call the `RHIPE` library in R, and initialize it.  For details, see the installation instructions.

```{r eval=FALSE, tidy=FALSE}
library(Rhipe)
rhinit()
rhoptions(zips = "/user/share/RhipeLib.tar.gz")
rhoptions(runner = "sh ./RhipeLib/library/Rhipe/bin/RhipeMapReduce.sh")
```

Now write the raw text file to HDFS.  The function that writes files to HDFS is `rhput()`.  
```{r eval=FALSE, tidy=FALSE}
rhput("/home/tongx/housing.txt", "/user/tongx/housing/housing.txt")
```

In this function, we need two arguments or two two absolute file path.
The first argument is the absolute path to the file to be copied which is located on our current 
local working directory, and the second argument is the absolute HDFS path where the file will be written. 
`rhput` creates the file at destination, overwriting the destination if it already exists.
One of the great part of `Rhipe` package in R is that all the communication to the Hadoop from user
can be achieved just in R. We do not have to worry about hadoop command that do all different types 
of filesystem management or mapreduce job.

We can confirm that the housing data text file has been written to HDFS. 

```{r eval=FALSE, tidy=FALSE}
rhexists("/user/tongx/housing/housing.txt")
```
```
[1] TRUE
```

If we want to see more details about a file or directory on HDFS, we can use `rhls()`.
```{r eval=FALSE, tidy=FALSE}
rhls("/user/tongx/housing")
```
```
  permission owner      group     size          modtime                            file
1 -rw-rw-rw- tongx supergroup 11.82 mb 2014-09-17 11:11 /user/tongx/housing/housing.txt
```
`rhls()` is very similar to the bash command `ls`.  It will list all content under given address. 
We can see that `housing.txt` file with size 11.82Mb is located under `/user/tongx/housing/` 
on HDFS.

### Read and Divide by State ###

The first step in a D\&R analysis is to choose a division method and create subsets.  We'll
divide the housing data by state.  Rhipe allows us to read the raw text file on HDFS and 
create a copy of the data broken into subsets using a MapReduce job.  The original data file will remain on HDFS.
The data within each subset will be stored as R objects, rather than raw text.  This will 
allow us to apply subsequent data analytic and visual methods using familiar R commands.

In the MapReduce framework, we keep track of subsets as key-value pairs.  The key is a label for
the subset.  It's usually a small data object, like a single number or character string.  There
can be multiple subsets that share a common key.  The value is where we store the data.  For 
example, it may be an R data frame.

The map is a function applied to every key-value pair.  Its output is one or more key-value
pairs, where the key may or may not have changed from the input key.  The output key-value 
pairs of the map are sometimes called intermediate key-value pairs, because they will be the
input to the reduce function.  The intermediate key-value pairs are grouped by key, and then
the reduce function is applied to each group.  The output of the reduce function is again
one or more key-value pairs.  `Rhipe` allows us to write the map and reduce functions in
R code.

A valid MapReduce job in `Rhipe` consists of a map expression, an optional reduce expression, 
and a execution function `rhwatch()`.  If no reduce expression is given, the output of the
map is the final output.

#### Map ####

```{r eval =FALSE, tidy=FALSE}
map1 <- expression({
  lapply(seq_along(map.keys), function(r) {
    line = strsplit(map.values[[r]], ",")[[1]]
    key <- line[[3]]
    value <- as.data.frame(rbind(line[-3]), stringsAsFactors = FALSE)
    rhcollect(key, value)
  })
})
```

The input keys are line numbers in the housing data text file.  They are available as the list
object `map.keys`.  The values are the lines of text, which are in the list object `map.values`.
This is the default for text files.  For each input key-value pair, `rhcollect` emits an
intermediate key-value pair, where the key is the state name (the third field in the comma-separated
line) and the value is all other fields in the line, stored as a single row data frame.

#### Reduce ####

```{r eval=FALSE, tidy=FALSE}
reduce1 <- expression(
  pre = {
    tmp <- data.frame()
  },
  reduce = {
    tmp <- rbind(tmp, do.call(rbind, reduce.values))
  },
  post = {
    names(tmp) <- c(
      "fips", "county", "time", 
      "sold", "list", "selling"
    )
    tmp$list <- as.numeric(tmp$list)
    tmp$selling <- as.numeric(tmp$selling)
    rhcollect(reduce.key, tmp)
  }
)
```

In `RHIPE`, reduce is an R expression that is evaluated 
by `RHIPE` during the reduce stage. All intermediate key/value pairs from map stage that share same
key will be grouped together and processed to be applied reduce function. In reduce-pre session, we 
initialize an empty data.frame `tmp`. `reduce.key` is the shared key, and `reduce.values` is a list 
that includes all values corresponding to that unique `reduce.key`. In reduce-reduce session, we row 
combined all one-row data.frame, and then row combined it with `tmp`. Finally in reduce-post session, 
we assigned the name to each column, and force the two price variable to be numeric. The final 
key/value pair for each state will be collected in the end by using `rhcollect()`.

#### Execution Function ####

```{r eval=FALSE, tidy=FALSE}
mr1 <- rhwatch(
  map      = map1,
  reduce   = reduce1,
  input    = rhfmt("/user/tongx/housing/housing.txt", type = "text"),
  output   = rhfmt("/user/tongx/housing/bystate", type = "sequence"),
  mapred   = list(
    mapred.reduce.tasks = 10
  ),
  readback = FALSE
)
```

After the map and reduce expression, we are heading to the execution function of a mapreduce job in 
`RHIPE`. `rhwatch()` is a call that packages the MapReduce job which is sent to Hadoop. In `rhwatch()` 
function, we assign the map and reduce expression to `map` and `reduce` argument respectively. Input
and output argument in `rhwatch()` function is used to specify the path on HDFS of input file and 
output file respectively, and there are three types of file we can consider, text, sequence, and map 
file. `mapred` argument is a list that can be used to customize the Hadoop and `RHIPE` options. Here 
we specified the `mapred.reduce.tasks` to be 10, so the number of reduce tasks will be set to be 10. 
This number also is related to the number of output files, since each reduce task will generate one 
part of output file for the final output. 

Once we submit the job, in R console we can see that job running information is keeping popping 
out, which will be helpful for you to have some idea about the status of running job.

```
Saving 1 parameter to /tmp/rhipe-temp-params-bbb96e029776c9953476a54c74d9eaf7 (use rhclean to delete all temp files)
[Thu Sep 18 22:25:52 2014] Name:2014-09-18 22:25:52 Job: job_201405301308_4709  State: PREP Duration: 0.203
URL: http://deneb.stat.purdue.edu:50030/jobdetails.jsp?jobid=job_201405301308_4709
       pct numtasks pending running complete killed failed_attempts killed_attempts
map      0        1       1       0        0      0               0               0
reduce   0       10      10       0        0      0               0               0
Waiting 5 seconds
[Thu Sep 18 22:25:57 2014] Name:2014-09-18 22:25:52 Job: job_201405301308_4709  State: RUNNING Duration: 5.24
URL: http://deneb.stat.purdue.edu:50030/jobdetails.jsp?jobid=job_201405301308_4709
       pct numtasks pending running complete killed failed_attempts killed_attempts
map      0        1       0       1        0      0               0               0
reduce   0       10      10       0        0      0               0               0
Waiting 5 seconds
``` 

We can use `rhread()` to read back the result from HDFS. The object we read back into R is a list 
object with the length equal to the number of total key/value pairs. Each element in this list is
also a list with length 2. The first one is the key, and the second one is the value. This is how 
`RHIPE` translate key/value pairs into R.

```{r eval=FALSE, tidy=FALSE}
rst <- rhread("/user/tongx/housing/bystate")
```
```
Read 49 objects(13.52 MB) in 1.41 seconds
```
```{r eval=FALSE, tidy=FALSE}
str(rst)
```
```
List of 49
 $ :List of 2
  ..$ : chr "WV"
  ..$ :'data.frame':    3836 obs. of  6 variables:
  .. ..$ fips   : chr [1:3836] "54015" "54015" "54015" "54015" ...
  .. ..$ county : chr [1:3836] "Clay" "Clay" "Clay" "Clay" ...
  .. ..$ time   : chr [1:3836] "2012-12-01" "2012-11-01" "2012-10-01" "2012-09-01" ...
  .. ..$ sold   : chr [1:3836] "NA" "NA" "NA" "NA" ...
  .. ..$ list   : num [1:3836] 78.5 80 80 80.1 93.1 ...
  .. ..$ selling: num [1:3836] NA NA NA NA NA NA NA NA NA NA ...
 $ :List of 2
  ..$ : chr "KY"
  ..$ :'data.frame':    8059 obs. of  6 variables:
  .. ..$ fips   : chr [1:8059] "21235" "21235" "21235" "21235" ...
  .. ..$ county : chr [1:8059] "Whitley" "Whitley" "Whitley" "Whitley" ...
  .. ..$ time   : chr [1:8059] "2012-05-01" "2012-07-01" "2012-08-01" "2012-09-01" ...
  .. ..$ sold   : chr [1:8059] "NA" "NA" "NA" "NA" ...
  .. ..$ list   : num [1:8059] 73.2 74.8 74.1 74.8 77.2 ...
  .. ..$ selling: num [1:8059] NA NA NA NA NA NA NA NA NA NA ...
......
 $ :List of 2
  ..$ : chr "TX"
  ..$ :'data.frame':    13108 obs. of  6 variables:
  .. ..$ fips   : chr [1:13108] "48215" "48479" "48479" "48479" ...
  .. ..$ county : chr [1:13108] "Hidalgo" "Webb" "Webb" "Webb" ...
  .. ..$ time   : chr [1:13108] "2009-05-01" "2011-11-01" "2011-12-01" "2012-01-01" ...
  .. ..$ sold   : chr [1:13108] "NA" "NA" "NA" "NA" ...
  .. ..$ list   : num [1:13108] 76.3 83.6 84.7 85.2 86.4 ...
  .. ..$ selling: num [1:13108] NA NA NA NA NA NA NA NA NA NA ...
 $ :List of 2
  ..$ : chr "WA"
  ..$ :'data.frame':    3705 obs. of  6 variables:
  .. ..$ fips   : chr [1:3705] "53049" "53049" "53049" "53049" ...
  .. ..$ county : chr [1:3705] "Pacific" "Pacific" "Pacific" "Pacific" ...
  .. ..$ time   : chr [1:3705] "2012-08-01" "2012-09-01" "2012-09-01" "2012-10-01" ...
  .. ..$ sold   : chr [1:3705] "NA" "29" "NA" "NA" ...
  .. ..$ list   : num [1:3705] 120 NA 125 NA 120 ...
  .. ..$ selling: num [1:3705] NA 94.6 NA 90.8 NA ...
```

So, as we expect, the `rst` is a list with length 49, and each element of `rst` is also a list with
two elements, one is the state abbreviation, another one is the data frame of the data for the corresponding
state.

### Compute State  Means ###

Now let's start to combine the result from the divide stage.

#### Map ####

```{r eval =FALSE, tidy=FALSE}
map2 <- expression({
  lapply(seq_along(map.keys), function(r) {
    value <- data.frame(
      state = map.keys[[r]],
      listmean = mean(map.values[[r]]$list, na.rm = TRUE),
      sellingmean = mean(map.values[[r]]$selling, na.rm = TRUE),
      stringsAsFactors = FALSE
    )
    rhcollect(1, value)
  })
})
```

In this map expression, we read in the key/value pairs we created in previous division. And for 
each state, means were calculated. We created a data.frame with one row, three columns which are
state, listmean, and sellingmean. When we calculated the mean, we removed all NA in the data. Then we
collected the key/value pair which key is 1, value is the data frame. The reason we assign 1 as key
to all state is that by doing this, we can collect all means by state together in the reduce stage,
and then create a final data frame.

#### Reduce ####

```{r eval=FALSE, tidy=FALSE}
reduce2 <- expression(
  pre = {
    combine <- data.frame()
  },
  reduce = {
    combine <- rbind(combine, do.call(rbind, reduce.values))
  },
  post = {
    rhcollect(reduce.key, combine)
  }
)
```

In the reduce expression, we still have three parts, pre, reduce, and post. In reduce-pre, we 
initialized a data frame `combine`. Since all key now is 1, all those data frames with one row, 
three columns will be grouped together as a list in `reduce.values`. After the row combined 
function, the `combine` object will be a data frame with all means for each state. Finally in
reduce-post, we collect the key which is 1, and the value which is the `combine`, and save this
key/value pair to HDFS.

#### Execution Function ####

In `rhwatch()` this time, we changed several arguments. First, in the `rhfmt` of `input` argument,
`type` is specified to be "sequence", since the input file to this mapreduce job is the output
from our division. Also we changed the `mapred.reduce.tasks` to be 5. At last, we assigned 
`readback` to be `TRUE`. By doing this, the final results will not only be saved on HDFS, but also
be read back from HDFS (without using `rhread()`) and assigned to an object in R, like `mr2`.

```{r eval=FALSE, tidy=FALSE}
mr2 <- rhwatch(
  map      = map2,
  reduce   = reduce2,
  input    = rhfmt("/user/tongx/housing/bystate", type = "sequence"),
  output   = rhfmt("/user/tongx/housing/meanbystate", type = "sequence"),
  mapred   = list( 
    mapred.reduce.tasks = 5
  ),
  readback = TRUE
)
```

Same as before, once we submit the job, in R console we can see job running information.

```
[Thu Sep 18 23:48:19 2014] Name:2014-09-18 23:48:19 Job: job_201405301308_4715  State: PREP Duration: 0.175
URL: http://deneb.stat.purdue.edu:50030/jobdetails.jsp?jobid=job_201405301308_4715
       pct numtasks pending running complete killed failed_attempts killed_attempts
map      0       10      10       0        0      0               0               0
reduce   0        5       5       0        0      0               0               0
Waiting 5 seconds
[Thu Sep 18 23:48:24 2014] Name:2014-09-18 23:48:19 Job: job_201405301308_4715  State: RUNNING Duration: 5.206
URL: http://deneb.stat.purdue.edu:50030/jobdetails.jsp?jobid=job_201405301308_4715
       pct numtasks pending running complete killed failed_attempts killed_attempts
map      0       10       0      10        0      0               0               0
reduce   0        5       5       0        0      0               0               0
Waiting 5 seconds
.......
Read 1 objects(2.42 KB) in 0.06 seconds
```

```{r eval=FALSE, tidy=FALSE}
str(mr2)
```
```
List of 1
 $ :List of 2
  ..$ : num 1
  ..$ :'data.frame':  49 obs. of  3 variables:
  .. ..$ state      : Factor w/ 49 levels "IA","KS","AZ",..: 1 2 3 4 5 6 7 8 9 10 ...
  .. ..$ listmean   : num [1:49] 74.5 66 110.6 369.1 151 ...
  .. ..$ sellingmean: num [1:49] 95.1 NaN 101.3 422.5 143.4 ...
```

As we can see, the class of the result is a list with one element which is a nested list with two 
elements. The first element is the key 1 which is meaningless, the second element is a data.frame with
three columns which are state abbreviation, mean of median list price per square feet, and mean of 
median sold price per square feet.
 
```{r eval=FALSE, tidy=FALSE}
data <- mr2[[1]][[2]][with(mr2[[1]][[2]], order(listmean, decreasing=TRUE)),]
head(data)
```
```
   state listmean sellingmean
4     DC 369.0546    422.4744
45    MA 259.1316    210.8714
44    CA 192.6343    185.8716
10    RI 188.9953    174.9670
46    NJ 180.0136    171.4495
22    CT 162.4686    151.7553
```

If you are wondering how the results looks like on HDFS, or how the results were saved on HDFS. 
We can actually have a look at it by using `rhls()` function.

```{r eval=FALSE, tidy=FALSE}
rhls("/user/tongx/housing/meanbystate")
```
```
  permission owner      group        size          modtime                                         file
1 -rw-r--r-- tongx supergroup           0 2014-09-18 23:56     /user/tongx/housing/meanbystate/_SUCCESS
2 drwxrwxrwx tongx supergroup           0 2014-09-18 23:56        /user/tongx/housing/meanbystate/_logs
3 -rw-r--r-- tongx supergroup    1.363 kb 2014-09-18 23:56 /user/tongx/housing/meanbystate/part-r-00000
4 -rw-r--r-- tongx supergroup    94 bytes 2014-09-18 23:56 /user/tongx/housing/meanbystate/part-r-00001
5 -rw-r--r-- tongx supergroup    94 bytes 2014-09-18 23:56 /user/tongx/housing/meanbystate/part-r-00002
6 -rw-r--r-- tongx supergroup    94 bytes 2014-09-18 23:56 /user/tongx/housing/meanbystate/part-r-00003
7 -rw-r--r-- tongx supergroup    94 bytes 2014-09-18 23:56 /user/tongx/housing/meanbystate/part-r-00004
```

So there are five files in output "/user/tongx/housing/meanbystate", named from "part-r-00000" to 
"part-r-00004". Besides these five files, there are another two files named "_SUCCESS" and 
"_logs" which records the metadata and log information. One more thing should be noticed here,
among those five output part files, there are four of them with the same size 94 bytes. This is
not by coincidence, and it means those four files are empty. Since we only have one key/value
pair and the key/value pair is the finest process unit in `HDFS`, only one reduce task was working
on that key/value pair, and the rest of four tasks are just empty.


### Read and Divide by County ###

The task for calculating the mean by state is finished, but what if we want to calculate the mean
by county over the time? Yes, the answer would be no problem. We will read in the txt file data again, 
and divide it by county instead of state. Same as before, we will illustrate a mapreduce job in `RHIPE`
to achieve this division.

#### Map ####

```{r eval =FALSE, tidy=FALSE}
map3 <- expression({
  lapply(seq_along(map.keys), function(r) {
    line = strsplit(map.values[[r]], ",")[[1]]
    key <- line[[1]]
    value <- as.data.frame(rbind(line[-1]), stringsAsFactors = FALSE)
    rhcollect(key, value)
  })
})
```

In the map expression, we still split each line of text file by a comma separator, then save the
first element of `line` which is the FIPS county code to be the key, and the rest columns of the
row are saved as a data.frame to be the value.

#### Reduce ####

```{r eval=FALSE, tidy=FALSE}
reduce3 <- expression(
  pre = {
    tmp <- data.frame()
  },
  reduce = {
    tmp <- rbind(tmp, do.call(rbind, reduce.values))
  },
  post = {
    names(tmp) <- c(
      "county", "state", "time", 
      "sold", "list", "selling"
    )
    state <- unique(tmp$state)
    county <- unique(tmp$county)
    tmp <- tmp[, !(names(tmp)%in%c("state", "county"))]
    attr(tmp, "state") <- state
    attr(tmp, "county") <- county
    tmp$list <- as.numeric(tmp$list)
    tmp$selling <- as.numeric(tmp$selling)
    rhcollect(reduce.key, tmp)
  }
)
```

The difference between this reduce expression than the one in divide in state session is that we
tried to minimize the size of the value objects. For a given county, the `state` column has all
same value as well as `county` column, so it is meaningless to keep the whole column in stead of 
just one string value. So in the reduce-post, once we got data.frame `tmp` for one county, we 
removed the `state` and `county` columns, and then created two attributes for `tmp` for recording 
state and county name information. By doing this, we eliminated redundant information in each 
data.frame. Imagine if we have a data.frame related to each key that has thousands of rows, keeping 
the value size as small as possible can save a lot of I/O time and transporting time between map 
and reduce.

#### Execution Function ####

```{r eval=FALSE, tidy=FALSE}
mr3 <- rhwatch(
  map      = map3,
  reduce   = reduce3,
  input    = rhfmt("/user/tongx/housing/housing.txt", type = "text"),
  output   = rhfmt("/user/tongx/housing/bycounty", type = "sequence"),
  mapred   = list(
    mapred.reduce.tasks = 10
  ),
  readback = FALSE
)
```

Again, after the job is done, we can use `rhread()` function. We specified another argument in 
`rhread()`, which is `max`. It can control how many key/value pairs we are reading in. The default
is -1, which means read in all key/value pairs.

```{r eval=FALSE, tidy=FALSE}
rst <- rhread("/user/tongx/housing/bycounty", max = 10)
```
```
Read 10 objects(31.39 KB) in 0.04 seconds
```
We also can grab all keys in the result. Recall that the result is a list of key/value pairs which
are also a list with two elements. We can use `lapply` to get all the first elements in the list.

```{r eval=FALSE, tidy=FALSE}
keys <- unlist(lapply(rst, "[[", 1))
keys
```
```
 [1] "01013" "01031" "01059" "01077" "01095" "01103" "01121" "04001" "05019" "05037"
```

Finally, let's check if we do have the state and county name information as an attribute saved with
the data frame.

```{r eval=FALSE, tidy=FALSE}
attributes(rst[[1]][[2]])
```
```
$names
[1] "time"             "sold"            "list"             "selling"

$row.names
 [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32
[33] 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 
[65] 65 66

$state
[1] "AL"

$county
[1] "Butler"

$class
[1] "data.frame"
```

### Compute County Means ###

Next, we calculate the means of list price and sold price for each county. It will be similar to 
what we have done with means by state but with a little update.

#### Map ####

In map expression, we will create a one-row data.frame for each county. The data.frame has five
columns: fips, listmean, sellingmean, state, and county name. State and county name information 
can be found in the attribute of each element of `map.values`. In order to combine all means by
county into one data.frame, we will assign 1 to be the key for all intermediate key/value pairs.

```{r eval=FALSE, tidy=FALSE}
map4 <- expression({
  lapply(seq_along(map.keys), function(r) {
    value <- data.frame(
      fips = map.keys[[r]],
      listmean = mean(map.values[[r]]$list, na.rm = TRUE),
      sellingmean = mean(map.values[[r]]$selling, na.rm = TRUE),
      stringsAsFactors = FALSE
    )
    value$state <- attr(map.values[[r]], "state")
    value$county <- attr(map.values[[r]], "county")
    rhcollect(1, value)
  })
})
```

#### Reduce ####

We can use the same reduce expression as we used in "Mean by State" session. The final output is 
consist of one key/value pair, which the key is 1, and value is the data.frame with all county
means.

```{r eval=FALSE, tidy=FALSE}
reduce4 <- expression(
  pre = {
    combine <- data.frame()
  },
  reduce = {
    combine <- rbind(combine, do.call(rbind, reduce.values))
  },
  post = {
    rhcollect(reduce.key, combine)
  }
)
```

#### Execution Function ####

```{r eval=FALSE, tidy=FALSE}
mr4 <- rhwatch(
  map      = map4,
  reduce   = reduce4,
  input    = rhfmt("/user/tongx/housing/bycounty", type = "sequence"),
  output   = rhfmt("/user/tongx/housing/meanbycounty", type = "sequence"),
  mapred   = list( 
    mapred.reduce.tasks = 1
  ),
  readback = TRUE
)
```

This time we specified the `mapred.reduce.tasks` to be only 1, since we know there is only one
key/value pair in the output, which means we only need one output file. Eliminating the number
of the final output files can speed up the job process time in the sense of eliminating log file
and metadata of the job.

```{r eval=FALSE, tidy=FALSE}
str(mr4)
```
```
List of 1
 $ :List of 2
  ..$ : num 1
  ..$ :'data.frame' :    2883 obs. of  5 variables:
  .. ..$ fips       : chr [1:2883] "01005" "01023" "01041" "01069" ...
  .. ..$ listmean   : num [1:2883] 85.5 54.3 59.3 87.6 74.4 ...
  .. ..$ sellingmean: num [1:2883] NaN NaN NaN NaN NaN ...
  .. ..$ state      : chr [1:2883] "AL" "AL" "AL" "AL" ...
  .. ..$ county     : chr [1:2883] "Barbour" "Choctaw" "Crenshaw" "Houston" ...
```
```{r eval=FALSE, tidy=FALSE}
head(rst[[1]][[2]])
```
```
   fips listmean sellingmean state       county
1 01005 85.46983         NaN    AL      Barbour
2 01023 54.27378         NaN    AL      Choctaw
3 01041 59.29840         NaN    AL     Crenshaw
4 01069 87.64237         NaN    AL      Houston
5 01087 74.35722         NaN    AL        Macon
6 01113 70.92100         NaN    AL      Russell
```

### New Division by State ###

In previous tasks, we started from reading in text data file. Input key/value pairs are each row
from the text file. It is also reasonable and common that we transfer from one type of division to a
different type of division. For example, we would like to divide the data set by state based on the
division by county.

#### Map ####

```{r eval=FALSE, tidy=FALSE}
map5 <- expression({
  lapply(seq_along(map.keys), function(r) {
    key <- attr(map.values[[r]], "state")
    county <- attr(map.values[[r]], "county")
    value <- map.values[[r]]
    value$fips <- rep(map.keys[[r]], nrow(value))
    value$county <- rep(county, nrow(value))
    rhcollect(key, value)
  })
})
```

In map expression this time, the input keys are county fips code, and values are data.frame of subset
for the county. State and county name information is in attribute of each data.frame. The output key
is the state abbreviation string, and output value is map input value with two more columns: fips 
code and county name.

#### Reduce ####

Same reduce expression as before, we just combine all data.frames that belong to the same state.

```{r eval=FALSE, tidy=FALSE}
reduce5 <- expression(
  pre = {
    combine <- data.frame()
  },
  reduce = {
    combine <- rbind(combine, do.call(rbind, reduce.values))
  },
  post = {
    rhcollect(reduce.key, combine)
  }
)
```

#### Execution Function ####

```{r eval=FALSE, tidy=FALSE}
mr5 <- rhwatch(
  map      = map5,
  reduce   = reduce5,
  input    = rhfmt("/user/tongx/housing/bycounty", type = "sequence"),
  output   = rhfmt("/user/tongx/housing/countytostate", type = "sequence"),
  mapred   = list( 
    mapred.reduce.tasks = 10
  ),
  readback = FALSE,
  noeval = TRUE
)
```

Here, we specified one more argument in `rhwatch()`, `noeval` argument means no evaluation. If it 
is `TRUE`, the job will not run. `rhwatch` will just return a job object. Let's see what `mr5` looks
like:

```{r eval=FALSE, tidy=FALSE}
str(mr5)
```
```
List of 3
 $ lines     :List of 295
  ..$ rhipe_reduce_justcollect                                 : chr "FALSE"
  ..$ rhipe_reduce                                             : chr "A\n2\n196865\n131840\n518\n1026\n1\n262153\n6\nsrcref\n19\n2\n781\n8\n5\n12\n5\n12\n12\n12\n5\n5\n1026\n1\n262153\n7\nsrcfile\n"| __truncated__
  ..$ rhipe_reduce_prekey                                      : chr "A\n2\n196865\n131840\n518\n1026\n1\n262153\n6\nsrcref\n19\n2\n781\n8\n2\n9\n2\n9\n9\n9\n2\n2\n1026\n1\n262153\n7\nsrcfile\n4\n0"| __truncated__
......
  ..$ mapred.job.shuffle.merge.percent                         : chr "0.66"
  ..$ mapred.map.max.attempts                                  : chr "4"
  ..$ jobclient.progress.monitor.poll.interval                 : chr "1000"
  ..$ dfs.safemode.extension                                   : chr "30000"
  .. [list output truncated]
 $ temp      : chr "/tmp/Rtmp7T9QVe/rhipe5f563a4100f1"
 $ parameters:List of 2
  ..$ envir:<environment: 0x9fe5590> 
  ..$ file : chr "/tmp/rhipe-temp-params-db4fcae80c0979c459c868a8db58ac50"
 - attr(*, "class")= chr "rhmr"
```

`mr5` is a list with three elements, `lines` element is a list with length 295 which includes all
`RHIPE` and Hadoop mapreduce job arguments. `temp` element is a string which specifies the address
of temperary directory on each node of cluster. `parameters` element is a list with length 2. We wil
come back to this `parameters` in later session. The class of `mr5` is `rhmr` which means `RHIPE` 
mapreduce job object.

Then if we want to run the job, we have to call `rhex()` function in `RHIPE`.

```{r eval=FALSE, tidy=FALSE}
job <- rhex(mr5, async = FALSE)
```

The first argument is a job object, and the second argument `async` is used to specify if function 
returns immediately, leaving the job running asynchronously in the background on Hadoop, and `FALSE`
means do not returns immediately. Once the job is finished, we can have a look at the `job` object.

```{r eval=FALSE, tidy=FALSE}
job
```
```
$state
[1] TRUE

$counters
$counters$FileSystemCounters
   FILE_BYTES_READ FILE_BYTES_WRITTEN    HDFS_BYTES_READ HDFS_BYTES_WRITTEN 
          14846061           31179682            8781400           14180224 

$counters$`Job Counters `
                                              Data-local map tasks 
                                                                10 
   Total time spent by all maps waiting after reserving slots (ms) 
                                                                 0 
Total time spent by all reduces waiting after reserving slots (ms) 
                                                                 0 
                                                 SLOTS_MILLIS_MAPS 
                                                             93603 
                                              SLOTS_MILLIS_REDUCES 
                                                            149521 
                                                Launched map tasks 
                                                                10 
                                             Launched reduce tasks 
                                                                10 

$counters$`Map-Reduce Framework`
             Combine input records             Combine output records Total committed heap usage (bytes) 
                                 0                                  0                        10083368960 
               CPU time spent (ms)                  Map input records                   Map output bytes 
                             87810                               2883                           14834469 
                Map output records   Physical memory (bytes) snapshot                Reduce input groups 
                              2883                         3740327936                                 49 
              Reduce input records              Reduce output records               Reduce shuffle bytes 
                              2883                                 49                           14846601 
                   Spilled Records                    SPLIT_RAW_BYTES    Virtual memory (bytes) snapshot 
                              5766                               1350                        33988882432 

$counters$rhipeInternal
partialFlush 
          10 

$counters$rhipe_timing
overall_mapper_ms 
            15736 

$counters$job_time
[1] 30.048

```
Mapreduce job information is returned here.

What if we specify the `async` to be `TRUE`? Let's check it out.

```{r eval=FALSE, tidy=FALSE}
job2 <- rhex(mr5, async = TRUE)
```

We found that the function returns immediately, and let the job running on Hadoop. Then how can we
check the status of the job? The answer is using `rhstatus()`.

```{r eval=FALSE, tidy=FALSE}
rhstatus(job2)
```
```
[Sat Sep 20 17:22:04 2014] Name:2014-09-20 17:21:58 Job: job_201405301308_4741  State: RUNNING Duration: 5.457
URL: http://deneb.stat.purdue.edu:50030/jobdetails.jsp?jobid=job_201405301308_4741
       pct numtasks pending running complete killed failed_attempts killed_attempts
map      0       10       0      10        0      0               0               0
reduce   0       10      10       0        0      0               0               0
Waiting 5 seconds
[Sat Sep 20 17:22:09 2014] Name:2014-09-20 17:21:58 Job: job_201405301308_4741  State: RUNNING Duration: 10.491
URL: http://deneb.stat.purdue.edu:50030/jobdetails.jsp?jobid=job_201405301308_4741
       pct numtasks pending running complete killed failed_attempts killed_attempts
map      0       10       0      10        0      0               0               0
reduce   0       10      10       0        0      0               0               0
Waiting 5 seconds
```

Job running information is back on screen, and also the job summary information is returned in the end.

```
$state
[1] "SUCCEEDED"

$duration
[1] 30.758

$progress
       pct numtasks pending running complete killed failed_attempts killed_attempts
map      1       10       0       0       10      0               0               0
reduce   1       10       0       0       10      0               0               0

$warnings
NULL
......
```
More job detailed information can be addressed by using `rhJobInfo()`.
```{r eval=FALSE, tidy=FALSE}
rhJobInfo(job2)
```

Finally, the result can be read back as a list object in R.

```{r eval=FALSE, tidy=FALSE}
rst <- rhread("/user/tongx/housing/countytostate")
```
```
Read 49 objects(15.22 MB) in 1.32 seconds
```
