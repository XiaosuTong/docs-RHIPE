## A Cluster Performance Experiment ##

### Introduction ###

Actually, the elapsed time depends on many factors. This presents an opportunity for optimizing the 
computation even further by making the best choice of the factor. Our approach to the optimization 
is to run statistically designed experiments. This experiment consists of a lot of factors. There are
two different types of elapsed time, which means there is one computation-type categorical factor. 
Three statistical factors  are "n", "m" and "v", which are the kernel factors for our experiment. 
There are also two Hadoop HDFS factors and two Hadoop MapReduce factors, two hardware factors. In 
total, we have 10 factors. We also have replicates for each combination of these factors. For more 
details, you can check the paper(link) which describes the whole experiment process. In this tutorial,
we mainly focus on the computation-type category factor "type" and the three statistical factors "n", 
"m" and "v".

We will vary the values of `n`, `m`, `v` and set 3 replicates for the experiment. 

- n    : 21, 23, 25, 27
- m    : 8, 9, 10, ..., 16
- v    : 4, 5, 6
- type : "O", "T", two levels
- rep  : 3

Firstly, we will run a serial computation in R, which means there is no parallel computing. By contrast,
another designed experiment with tessera computation system will be carried out to check the 
computation ablity of tessera. We have shown how to run a single run of the performance test, so we 
can easily generalize the single run to the whole cluster performance experiment.

### Serial Computation in R ###

Let's see what we can do without tessera computation environment, and we only run this experiment in
R.

```{r, eval=FALSE,tidy=FALSE}
m.vec <- c(2^21, 2^23, 2^25, 2^27)
v.vec <- c(4, 5, 6)
p.vec <- 2^v.vec -1
rep.vec = 1:3
timing = data.frame()
for (rep in rep.vec){
  for (m in m.vec){
    for (p in p.vec){
      value  = matrix(c(rnorm(m*p), sample(c(0,1), m, replace=TRUE)), ncol=p+1)
      t      = system.time(glm.fit(value[,1:p],value[,p+1],family=binomial())$coef)[3]
      t      = data.frame(rep=rep,n=n,v=v,m=m,t=t)
      timing = rbind(timing,t)
      
    }
  }
}
```

### A Designed Experiment Using RHIPE ###
#### Generation Datasets Example ####

```{r,eval=FALSE,tidy=FALSE}
n.vec   <- c(21,23,25,27)
m.vec   <- seq(8, 16, by=1)
v.vec   <- 4:6
p.vec   <- 2^v.vec - 1
run.vec <- 3
dir.exp <- "/ln/song273/tmp/multi.factor"
```

First, we specify some parameters in the frond end of R. The `run.vec` denotes for each combination 
of `m`,`v`, we have 3 replicates. The `dir.exp` specifies the directory for this experiment on HDFS.

##### Map and Execution Function #####
```{r, eval=FALSE,tidy=FALSE}
for (n in n.vec){
 for (m in m.vec) {
  for (p in p.vec) {
    dir.dm = paste(dir.exp,"/dm/",'n',n,'p',p,'m',m, sep="")
    map1
    mr1
  }
 }
}
```

The `map1` and `mr1` are the map function and the execution function seperately we write in the datasets 
generation part of the single run example. There is a single run for each fixed value of `m` and `p`,
So the idea is to put the single run code to a for-loop. And the for-loop contains multinumber of 
MapReduce jobs. The `dir.dm` specifies the output location for each MapReduce job. 

It is noteworthy that we only generate subsets once and in the following timing part, we would read
the subsets from HDFS to the front end of R 3 times. 

##### Elapsed Time Measurement #####

```{r, eval=FALSE,tidy=FALSE}
for (n in n.vec){
 for (rep in rep.vec) {
  
  ## timing for O
  type = "O"
  for (m in m.vec) {
    for (v in v.vec) {
      dir.dm = paste(dir.exp,"/dm/",'n',n,'v',v,"m",m, sep="")
      dir.nf = paste(dir.exp,"/nf/","rep",rep,'n',n,'v',v,"m",m, sep="")
      map2
      mr2
      t      = as.numeric(system.time({rhex(mr2, async=FALSE)})[3])
      t      = data.frame(rep=rep,n=n,m=m,v=v,type=type,t=t)
      timing = rbind(timing,t) 
    }
  }
  ## timing for T
  type = "T"
   for (m in m.vec) {
    for (v in v.vec) {
      dir.dm = paste(dir.exp,"/dm/",'n',n,'v',v,"m",m, sep="")
      dir.gf = paste(dir.exp,"/gf/","rep",rep,'n',n,'v',v,"m",m, sep="")
      mp3
      reduce3
      mr3
      t      = as.numeric(system.time({rhex(mr3, async=FALSE)})[3])
      t      = data.frame(rep=rep,n=n,m=m,v=v,type=type,t=t)
      timing = rbind(timing,t)   
    }
  }
 }
}
```

The first for-loop stands for the different values of `n`. For each fixed value of `n`, the second 
for-loop stands for the replicates. For each fixed value of `n` and `rep`, the third and forth 
for-loop is for differente combination of `m` and `v`. There is a single run of performance test for
any fixed `n`, `rep`, `m` and `v`. `dir.dm` and `dir.nf` seperately specify the input type and output
type for execution function `mr2`. The `dir.dm` and `dir.gf` seperately specify the input type and 
output type for execution function `mr3`. The `map2`, `map3`, `reduce3` are the map and reduce 
function we specified before.

Finally, we save our final results to a data frame called `timing`.

#### Results of the Experiment of the Performance Test ####

```{r, eval=FALSE,tidy=FALSE}
timing
```
```
    rep  n  m v type        t
1     1 21  8 4    O   20.644
2     1 21  8 5    O   19.601
3     1 21  8 6    O   21.530
4     1 21  9 4    O   19.581
5     1 21  9 5    O   19.521
6     1 21  9 6    O   21.493
7     1 21 10 4    O   19.571
8     1 21 10 5    O   19.514
9     1 21 10 6    O   22.500
10    1 21 11 4    O   19.499
11    1 21 11 5    O   22.517
12    1 21 11 6    O   21.513
13    1 21 12 4    O   20.011
14    1 21 12 5    O   18.531
15    1 21 12 6    O   22.004
16    1 21 13 4    O   19.623
17    1 21 13 5    O   19.497
18    1 21 13 6    O   22.654
19    1 21 14 4    O   19.473
20    1 21 14 5    O   19.472
...

```
The `timing` dataframe has 648 rows and 6 collumns because `n` has 4 different values, `type` has two levels, `m` has 9 different values and `v` has 3 different values. For each combination of the factors, we have 3 replicates. So we have $4*2*9*3*3=648$ rows. 

##### Save the Results to the local #####

Because the size of data frame `timing` is not very large, we can easily download the dataset to our local computer. But first, we need to save `timing` to the server front end.
Then we will copy this file from server to our local computer.

```{r, eval=FALSE, tidy=FALSE }
dir = "/ln/song273/tmp"
dir.local = "/home/median/u41/song273/timetest/"

```

The `dir` specifies the top level directory for experiment on HDFS. The `dir.local` specifies the 
directory for local cluster file system.

```{r, eval=FALSE, tidy=FALSE }
rhsave(timing, file=paste(dir, "/save/", "timing", ".RData", sep=""))
save(timing, file=paste(dir.local, "timing", ".RData", sep=""))
```

The RHIPE function `rhsave` helps save the .RData file to the directory of `dir` on HDFS with file 
name `timing.RData`. The R function `save` write the `timing.RData` file to the local cluster file
Then we can use linux command `scp` to copy the file from the local cluster file to our local 
computer file.

```{r, eval=FALSE, tidy=FALSE }
scp song273@deneb.stat.purdue.edu:timing.RData /home/song273/elapsedtime
```

The linux command `scp` will copy the `timing.RData` file from the remote host cluster to the local
host under the directory `elapsedtime`. Then we can analyse the data in our local computer which 
gives us more freedom to analysis, for examle we don't need to worry about the abrupt network fault.

#### Visualize the Results ####

Here we use R function `xyplot` to visualize the results.

- The first plot describes the relationship between two kinds of elapsed time and log subsets size `m`.

   [Plot1. Elapsed time plots against m](./plots/Elapsed_Time_m_v.pdf).

   As we can see from the first plot, for every fixed value of `n` and `v`, the total elapsed time 
   "T" decreases first and then increases later as the value of `m` changes. And the three different
   curves in one pannel show the same drop-increase pattern.

- The second plot describes the relationship between two kinds of elapsed time and log number of 
varibales `v`.

   [Plot2. Elapsed time plots against v](./plots/Elapsed_Time_v_n.pdf).

   As we can see from the plot2, for fixed value of `m` , as the value of `v` increases, the value 
   of log2(elapsed time) increases linearly in each panel of the plot.

- The third plot describes the relationship between two kinds of elapsed time and log observations `n`.

   [Plot3. Elapsed time plots against n](./plots/Elapsed_Time_n_v.pdf).

- The forth plot describes the relationship between two kinds of elapsed time and 3 replicates.

   [Plot4. Elapsed time plots against Replicates](./plots/Elapsed_Time_rep_v.pdf).

   This plots helps to see whether replication has blocking effect on the elapsed time. Because R has 
   temporary memory and we read the subsets from HDFS to the front end of R for the first time and R 
   has temperory memory about these files and in the next replicate of reading part, R maybe just 
   reads these subsets from memory instead of reading them from the HDFS, which will affect our 
   elpased time experiment.

   From the plot4, we can see that replication doesn't have the blocking effects.
  
