### Read and Divide by State ###

The first step in a D\&R analysis is to choose a division method and create subsets.  We'll
divide the housing data by state. `RHIPE` allows us to read the raw text file on HDFS and 
create a copy of the data broken into subsets using a MapReduce job.  The original data file
will remain on HDFS. The data within each subset will be stored as R objects, rather than raw
text.  This will allow us to apply subsequent data analytic and visual methods using familiar
R commands.

In the MapReduce framework, we keep track of subsets as key-value pairs.  The key is a identifier for
the subset.  It's usually a small data object, like a single number or character string.  There
can be multiple subsets that share a common key.  The value is where we store the data.  For 
example, it may be an R data frame.

The map is a function applied to every key-value pair.  Its output is one or more key-value
pairs, where the key may or may not have changed from the input key.  The output key-value 
pairs of the map are sometimes called intermediate key-value pairs, because they will be the
input to the reduce function.  The intermediate key-value pairs are grouped by key, and then
the reduce function is applied to each group.  The output of the reduce function is again
one or more key-value pairs.  `RHIPE` allows us to write the map and reduce functions in
R code, using R objects for the keys and values.

A valid MapReduce job in Rhipe consists of a map expression, an optional reduce expression, 
and an execution function `rhwatch()`.  If no reduce expression is given, the output of the
map is the final output.

#### Map ####

```{r eval =FALSE, tidy=FALSE}
map1 <- expression({
  lapply(seq_along(map.keys), function(r) {
    line = strsplit(map.values[[r]], ",")[[1]]
    key <- line[3]
    value <- as.data.frame(rbind(line[-3]), stringsAsFactors = FALSE)
    names(value) <- c(
      "FIPS", "county", "date", 
      "units", "list", "selling"
    )
    value$list <- as.numeric(value$list)
    value$selling <- as.numeric(value$selling)
    value$units <- as.numeric(value$units)
    rhcollect(key, value)
  })
})
```

The input keys are line numbers in the housing data text file.  They are the elements of the list
object `map.keys`.  The input values are the lines of text, which are the elements of the list 
object `map.values`. This is the default when the input is a raw text file.  For each input key-value 
pair, `rhcollect` emits an intermediate key-value pair, where the key is the state name (the third 
field in the comma-separated line) and the value is all other fields in the line, stored as a data 
frame with a single row.

#### Reduce ####

```{r eval=FALSE, tidy=FALSE}
reduce1 <- expression(
  pre = {
    oneState <- data.frame()
  },
  reduce = {
    oneState <- rbind(oneState, do.call(rbind, reduce.values))
  },
  post = {
    attr(oneState, "state") <- reduce.key
    rhcollect(reduce.key, oneState)
  }
)
```

Between the map and reduce stages, the intermediate key-value pairs are grouped by key (the state name).
The current group's key is available in the object `reduce.key`, and all values associated with that key
are elements of the list object `reduce.values`.  The reduce expression has three parts: `pre`, which is 
executed once first; `reduce`, which is executed repeatedly until all intermediate values associated with
the current key have been processed; and `post`, which is executed once at the end.  

In the reduce expression above, our goal is to combine all observations associated with one particular
state into a single data frame.  In `pre`, we initialize an empty data frame, `oneState`.  In `reduce`, 
we use `rbind`
to combine all observations associated with one particular state.  In `post`, we add an attribute to the
data frame containing the state name and emit the final key-value
pair.  The key is the state name, and the value is the data frame with all observations belonging to
that state.  These final key-value pairs are written to HDFS, and will persist for subsequent analyses.

#### Execution Function ####

```{r eval=FALSE, tidy=FALSE}
mr1 <- rhwatch(
  map      = map1,
  reduce   = reduce1,
  input    = rhfmt("/ln/tongx/housing/housing.txt", type = "text"),
  output   = rhfmt("/ln/tongx/housing/byState", type = "sequence"),
  mapred   = list(
    mapred.reduce.tasks = 10
  ),
  readback = FALSE
)
```

The `rhwatch` function packages and executes our `RHIPE` MapReduce job.  In addition to the `map` and
`reduce` expressions created above, we specify the HDFS locations of the input and output for this
MapReduce job.  The input is the location where we stored our raw text file using `rhput` in the
previous section.  The output is any location on HDFS we choose.  Be careful, as any existing data
in the output location will be overwritten.

The `mapred` argument contains optional configuration parameters for the MapReduce job.  In this
case, we've specified 10 reduce tasks using `mapred.reduce.tasks`.  This means that of the 49 groups
of key-value pairs corresponding to the 49 states in our data set, 10 at a time will be processed
in parallel.  Specifying 10 reduce tasks also means that the output written to HDFS will be broken
into 10 files (we'll come back to this point in the next section).  Finally, `readback = FALSE` 
tells `RHIPE` not to read the final output from HDFS into global environment of our interactive R 
session.  We'll do that with a separate command. 

```
Saving 1 parameter to /tmp/rhipe-temp-params-bbb96e029776c9953476a54c74d9eaf7 (use rhclean to delete all temp files)
[Thu Sep 18 22:25:52 2014] Name:2014-09-18 22:25:52 Job: job_201405301308_4709  State: PREP Duration: 0.203
URL: http://deneb.stat.purdue.edu:50030/jobdetails.jsp?jobid=job_201405301308_4709
       pct numtasks pending running complete killed failed_attempts killed_attempts
map      0        1       1       0        0      0               0               0
reduce   0       10      10       0        0      0               0               0
Waiting 5 seconds
[Thu Sep 18 22:25:57 2014] Name:2014-09-18 22:25:52 Job: job_201405301308_4709  State: RUNNING Duration: 5.24
URL: http://deneb.stat.purdue.edu:50030/jobdetails.jsp?jobid=job_201405301308_4709
       pct numtasks pending running complete killed failed_attempts killed_attempts
map      0        1       0       1        0      0               0               0
reduce   0       10      10       0        0      0               0               0
Waiting 5 seconds
``` 

After our job has completed successfully, the output will be in the location we specified on the HDFS.
Since this data set is quite small, we can read the whole thing from HDFS into our interactive R
environment using `rhread`.  All we have to specify is the HDFS location we wish to read from.

```{r eval=FALSE, tidy=FALSE}
stateSubsets <- rhread("/ln/tongx/housing/byState")
```
```
Read 49 objects(13.52 MB) in 1.41 seconds
```

`RHIPE` conveniently packages the key-value pairs in our HDFS output location as a nested list, which we've
assigned to the variable `stateSubsets`.  Since there were 49 key-value pairs in the output of our reduce
stage, there are 49 elements in `stateSubsets`.  Each element is itself a list with two elements: a key
and a value.  In this case, the keys are character strings containing the state names, and the 
values are data frames, just as they should be based on our reduce code.

We can take a look at the data frame contained in the first key-value pair.  The key-value pairs
are in no particular order.    

```{r eval=FALSE, tidy=FALSE}
head(stateSubsets[[1]][[2]])
```

We can also look at the structure of `stateSubsets` and confirm that it's what we expected.

```{r eval=FALSE, tidy=FALSE}
str(stateSubsets)
```

```
 $ :List of 2
  ..$ : chr "WV"
  ..$ :'data.frame':  3630 obs. of  6 variables:
  .. ..$ FIPS   : chr [1:3630] "54103" "54103" "54033" "54051" ...
  .. ..$ county : chr [1:3630] "Wetzel" "Wetzel" "Harrison" "Marshall" ...
  .. ..$ date   : chr [1:3630] "65" "66" "66" "1" ...
  .. ..$ units  : num [1:3630] NA NA NA NA NA NA NA NA NA NA ...
  .. ..$ list   : num [1:3630] 57.9 52.7 91.4 67 63.8 ...
  .. ..$ selling: num [1:3630] NA NA NA NA NA NA NA NA NA NA ...
  .. ..- attr(*, "state")= chr "WV"
 $ :List of 2
  ..$ : chr "KY"
  ..$ :'data.frame':  7458 obs. of  6 variables:
  .. ..$ FIPS   : chr [1:7458] "21173" "21173" "21155" "21155" ...
  .. ..$ county : chr [1:7458] "Montgomery" "Montgomery" "Marion" "Marion" ...
  .. ..$ date   : chr [1:7458] "2" "1" "66" "65" ...
  .. ..$ units  : num [1:7458] NA NA NA NA NA NA NA NA NA NA ...
  .. ..$ list   : num [1:7458] 76 80.2 60.6 71.5 72.5 ...
  .. ..$ selling: num [1:7458] NA NA NA NA NA NA NA NA NA NA ...
  .. ..- attr(*, "state")= chr "KY"
 $ :List of 2
  ..$ : chr "NV"
  ..$ :'data.frame':	1056 obs. of  6 variables:
  .. ..$ FIPS   : chr [1:1056] "32029" "32029" "32029" "32029" ...
  .. ..$ county : chr [1:1056] "Storey" "Storey" "Storey" "Storey" ...
  .. ..$ date   : chr [1:1056] "3" "4" "5" "6" ...
  .. ..$ units  : num [1:1056] NA NA NA NA NA NA 11 NA 11 12 ...
  .. ..$ list   : num [1:1056] 155 149 143 147 149 ...
  .. ..$ selling: num [1:1056] NA NA NA NA NA NA NA NA NA NA ...
  .. ..- attr(*, "state")= chr "NV"
......
```

We've now successfully divided our data into subsets and stored each subset as an R object.  
They will persist on HDFS and be used for many analytic methods, each applied using `RHIPE`. 